import json
from typing import List, Dict
from parser import ContentParser
from browser_handler import BrowserHandler

class WebScraper:
    """ä¸»è¦çš„ç½‘é¡µæŠ“å–å™¨"""
    
    def __init__(self, headless: bool = False):
        self.parser = ContentParser()
        self.browser = BrowserHandler(headless=headless)
        self.final_data = []
        self.bad_data = []
    
    def process_single_item(self, item: Dict) -> bool:
        """å¤„ç†å•ä¸ªé¡¹ç›®"""
        try:
            # 1. å¯¼èˆªåˆ°URL
            if not self.browser.navigate_to_url(item["doi"]):
                return False
            
            # 2. æ¨¡æ‹Ÿäººç±»è¡Œä¸º
            self.browser.simulate_human_scroll()
            self.browser.click_random_blank_area()

            # 3. æ£€æŸ¥URLé‡å®šå‘
            redirect_info = self.browser.check_url_redirect(item["doi"])
            if redirect_info["redirected"]:
                if redirect_info["issue"] == "ip_problem":
                    print(f"âŒ IPé—®é¢˜: {self.browser.get_current_url()}")
                    return False  # ä¸¥é‡é—®é¢˜ï¼Œåœæ­¢å¤„ç†
                else:
                    print(f"âš ï¸ {redirect_info['issue']}: {self.browser.get_current_url()}")
                    item["skip_reason"] = redirect_info["issue"]
                    self.bad_data.append(item)
                    return True  # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
            
            
            # 4. è·å–é¡µé¢å†…å®¹
            page_source = self.browser.get_page_source()
            if not page_source:
                print(f"âŒ æ— æ³•è·å–é¡µé¢å†…å®¹: {item['doi']}")
                return True
            
            # 5. éªŒè¯é¡µé¢å†…å®¹
            validation = self.parser.validate_page_content(page_source)
            if not validation["is_valid"]:
                print(f"âš ï¸ Skip: {validation['skip_reason']} -> {item['title']}")
                item["skip_reason"] = validation["skip_reason"]
                self.bad_data.append(item)
                return True
            
            # 6. è§£æå†…å®¹
            extracted_data = self.parser.parse_html(page_source)
            item.update(extracted_data)
            self.final_data.append(item)
            
            return True
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {item['doi']}ï¼ŒåŸå› : {e}")
            return False
    
    def process_items(self, items: List[Dict]) -> None:
        # 1. ç­›é€‰å‡ºæ‰€æœ‰Elsevierçš„æ¡ç›®
        elsevier_items = [item for item in items if item.get("publisher") == "Elsevier"]
        print(f"find {len(elsevier_items)} Elsevier items.")
        
        # 2. åŠ è½½å·²å¤„ç†çš„DOIåˆ—è¡¨
        processed_dois = set()
        
        # ä»æˆåŠŸå¤„ç†çš„æ–‡ä»¶ä¸­åŠ è½½DOI
        try:
            with open("./Data/elsevier_extracted.json", "r", encoding="utf-8") as f:
                extracted_data = json.load(f)
                for item in extracted_data:
                    if "doi" in item and item["doi"]:
                        processed_dois.add(item["doi"])
        except FileNotFoundError:
            print("ğŸ“‹ elsevier_extracted.json is not found, start from empty.")
            pass
        
        # ä»å¤±è´¥å¤„ç†çš„æ–‡ä»¶ä¸­åŠ è½½DOI
        try:
            with open("./Data/elsevier_bad.json", "r", encoding="utf-8") as f:
                bad_data = json.load(f)
                bad_dois_count = 0
                for item in bad_data:
                    if "doi" in item and item["doi"]:
                        processed_dois.add(item["doi"])
                        bad_dois_count += 1
        except FileNotFoundError:
            print("ğŸ“‹ bad.json is not found, start from empty.")
            pass
        
        # 3. ç­›é€‰å‡ºæœªå¤„ç†çš„æ¡ç›®
        unprocessed_items = []
        for item in elsevier_items:
            if item.get("doi") not in processed_dois:
                unprocessed_items.append(item)
        
        total_processed = len(processed_dois)
        total_unprocessed = len(unprocessed_items)
        
        print(f"Statistics:")
        print(f"   - Total Elsevier items: {len(elsevier_items)}")
        print(f"   - Processed items: {total_processed}")
        print(f"   - Unprocessed items: {total_unprocessed}")
        
        if total_unprocessed == 0:
            print("âœ… All Elsevier items are processed.")
            return
        
        # 4. å¤„ç†æœªå¤„ç†çš„æ¡ç›®
        print(f"ğŸš€ Start processing {total_unprocessed} unprocessed Elsevier items...")
        
        for i, item in enumerate(unprocessed_items, 1):
            print(f"\nProgress [{i}/{total_unprocessed}] - DOI: {item.get('doi', 'N/A')}")
            
            if not self.process_single_item(item):
                print(f"âŒ Encounter serious error, stop processing. Processed {i-1}/{total_unprocessed} items.")
                break  # é‡åˆ°ä¸¥é‡é”™è¯¯æ—¶åœæ­¢
    
    def save_results(self, 
                    final_data_file: str = "./Data/elsevier_extracted.json",
                    bad_data_file: str = "./Data/elsevier_bad.json"):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        # ä¿å­˜æˆåŠŸå¤„ç†çš„æ•°æ®
        try:
            with open(final_data_file, "r", encoding="utf-8") as f:
                original_data = json.load(f)
        except FileNotFoundError:
            original_data = []
        
        original_data.extend(self.final_data)
        with open(final_data_file, "w", encoding="utf-8") as f:
            json.dump(original_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è·³è¿‡çš„æ•°æ®
        try:
            with open(bad_data_file, "r", encoding="utf-8") as f:
                original_bad_data = json.load(f)
        except FileNotFoundError:
            original_bad_data = []
        
        original_bad_data.extend(self.bad_data)
        with open(bad_data_file, "w", encoding="utf-8") as f:
            json.dump(original_bad_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Process completed: {len(self.final_data)} items processed, {len(self.bad_data)} items skipped.")
    
    def close(self):
        """å…³é—­èµ„æº"""
        self.browser.close()
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½è¾“å…¥æ•°æ®
    with open("./Databases/publisher_doi.json", "r", encoding="utf-8") as f:
        input_json = json.load(f)
    
    # å¤„ç†æ•°æ®
    with WebScraper(headless=False) as scraper:
        scraper.process_items(input_json)
        scraper.save_results()

if __name__ == "__main__":
    main()