"""
ä¼˜åŒ–ç‰¹å¾é€‰æ‹©æ¨¡å—
åŸºäºPLSæµ‹è¯•ç»“æœçš„æœ€ä½³é…ç½®
"""

import pandas as pd
import numpy as np
from typing import Dict, Tuple, Optional
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.cross_decomposition import PLSRegression
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings('ignore')


class PLSTransformer(BaseEstimator, TransformerMixin):
    """PLSè½¬æ¢å™¨ï¼Œç”¨äºé™ç»´"""
    
    def __init__(self, n_components=2):
        self.n_components = n_components
        self.pls = PLSRegression(n_components=n_components)
        
    def fit(self, X, y):
        # å°†åˆ†ç±»æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼
        y_numeric = y.astype(int) if hasattr(y, 'astype') else np.array(y, dtype=int)
        self.pls.fit(X, y_numeric)
        return self
    
    def transform(self, X):
        # ä½¿ç”¨PLSå˜æ¢ç‰¹å¾ï¼Œåªè¿”å›Xçš„å˜æ¢ç»“æœ
        X_transformed = self.pls.transform(X)
        # ç¡®ä¿è¿”å›2Dæ•°ç»„
        if X_transformed.ndim > 2:
            X_transformed = X_transformed.reshape(X_transformed.shape[0], -1)
        return X_transformed
    

class OptimizedFeatureSelector:
    """ä¼˜åŒ–çš„ç‰¹å¾é€‰æ‹©å™¨"""
    
    def __init__(self):
        self.task_configs = {
            'rl_class': {
                'feature_method': 'mutual_info',
                'n_features': 50,
                'pls_components': 1,
                'description': 'Mutual Info (K=50) + PLS-1'
            },
            'eab_class': {
                'feature_method': 'rf_importance', 
                'n_features': 20,
                'pls_components': 1,
                'description': 'RF-Top20 + PLS-1'
            }
        }
        
        self.selectors = {}
        self.scalers = {}
        self.pls_transformers = {}
        self.classifiers = {}
    
    def _select_features_mutual_info(self, X_train, y_train, X_test, n_features):
        """ä½¿ç”¨äº’ä¿¡æ¯é€‰æ‹©ç‰¹å¾"""
        selector = SelectKBest(mutual_info_classif, k=n_features)
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_test_selected = selector.transform(X_test)
        
        # è½¬æ¢å›DataFrameä¿æŒåˆ—å
        selected_features = X_train.columns[selector.get_support()]
        X_train_selected = pd.DataFrame(X_train_selected, columns=selected_features, index=X_train.index)
        X_test_selected = pd.DataFrame(X_test_selected, columns=selected_features, index=X_test.index)
        
        return X_train_selected, X_test_selected, selector
    
    def _select_features_rf_importance(self, X_train, y_train, X_test, n_features):
        """ä½¿ç”¨éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§é€‰æ‹©ç‰¹å¾"""
        # è®­ç»ƒéšæœºæ£®æ—è·å–ç‰¹å¾é‡è¦æ€§
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X_train, y_train)
        
        # è·å–ç‰¹å¾é‡è¦æ€§æ’åº
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': rf.feature_importances_
        }).sort_values('importance', ascending=False)
        
        # é€‰æ‹©topç‰¹å¾
        top_features = feature_importance.head(n_features)['feature'].tolist()
        
        X_train_selected = X_train[top_features]
        X_test_selected = X_test[top_features]
        
        return X_train_selected, X_test_selected, rf
    
    def optimize_dataset_features(self, X_train, y_train, X_test, task_name):
        """
        ä¸ºç‰¹å®šä»»åŠ¡ä¼˜åŒ–ç‰¹å¾
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            X_test: æµ‹è¯•ç‰¹å¾
            task_name: ä»»åŠ¡åç§°
            
        Returns:
            ä¼˜åŒ–åçš„è®­ç»ƒå’Œæµ‹è¯•ç‰¹å¾
        """
        if task_name not in self.task_configs:
            print(f"âš ï¸ æœªæ‰¾åˆ°ä»»åŠ¡ {task_name} çš„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            task_name = 'rl_class'  # ä½¿ç”¨é»˜è®¤é…ç½®
        
        config = self.task_configs[task_name]
        print(f"ğŸ¯ ä½¿ç”¨æœ€ä½³é…ç½®: {config['description']}")
        
        # 1. æ ‡å‡†åŒ–ç‰¹å¾
        print(f"   ğŸ”§ æ ‡å‡†åŒ–ç‰¹å¾...")
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # è½¬æ¢å›DataFrame
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
        
        # 2. ç‰¹å¾é€‰æ‹©
        print(f"   ğŸ”§ ç‰¹å¾é€‰æ‹©: {config['feature_method']}, é€‰æ‹© {config['n_features']} ä¸ªç‰¹å¾")
        if config['feature_method'] == 'mutual_info':
            X_train_selected, X_test_selected, selector = self._select_features_mutual_info(
                X_train_scaled, y_train, X_test_scaled, config['n_features']
            )
        else:  # rf_importance
            X_train_selected, X_test_selected, selector = self._select_features_rf_importance(
                X_train_scaled, y_train, X_test_scaled, config['n_features']
            )
        
        # 3. PLSé™ç»´
        print(f"   ğŸ”§ PLSé™ç»´: {config['pls_components']} ä¸ªç»„ä»¶")
        pls_transformer = PLSTransformer(n_components=config['pls_components'])
        X_train_pls = pls_transformer.fit_transform(X_train_selected, y_train)
        X_test_pls = pls_transformer.transform(X_test_selected)
        
        # è½¬æ¢ä¸ºDataFrame
        pls_columns = [f'PLS_Component_{i+1}' for i in range(config['pls_components'])]
        X_train_final = pd.DataFrame(X_train_pls, columns=pls_columns, index=X_train.index)
        X_test_final = pd.DataFrame(X_test_pls, columns=pls_columns, index=X_test.index)
        
        # ä¿å­˜ç»„ä»¶
        self.scalers[task_name] = scaler
        self.selectors[task_name] = selector
        self.pls_transformers[task_name] = pls_transformer
        
        print(f"   âœ… ç‰¹å¾ä¼˜åŒ–å®Œæˆ: {X_train_final.shape[1]} ä¸ªPLSç»„ä»¶")
        print(f"   ğŸ“Š ç‰¹å¾èŒƒå›´: [{X_train_final.min().min():.3f}, {X_train_final.max().max():.3f}]")
        
        return X_train_final, X_test_final
    
    def create_optimized_pipeline(self, task_name):
        """åˆ›å»ºä¼˜åŒ–çš„é¢„æµ‹ç®¡é“"""
        if task_name not in self.task_configs:
            task_name = 'rl_class'
        
        config = self.task_configs[task_name]
        
        # åˆ›å»ºç®¡é“æ­¥éª¤
        steps = [
            ('scaler', StandardScaler()),
        ]
        
        # æ·»åŠ ç‰¹å¾é€‰æ‹©æ­¥éª¤
        if config['feature_method'] == 'mutual_info':
            steps.append(('feature_selector', SelectKBest(mutual_info_classif, k=config['n_features'])))
        else:
            # å¯¹äºRFé‡è¦æ€§ï¼Œéœ€è¦è‡ªå®šä¹‰é€‰æ‹©å™¨
            steps.append(('feature_selector', SelectKBest(mutual_info_classif, k=config['n_features'])))
        
        # æ·»åŠ PLSé™ç»´
        steps.append(('pls', PLSTransformer(n_components=config['pls_components'])))
        
        # æ·»åŠ åˆ†ç±»å™¨
        steps.append(('classifier', LogisticRegression(max_iter=1000)))
        
        pipeline = Pipeline(steps)
        
        return pipeline
    
    def get_feature_importance_analysis(self, X_train, y_train, task_name):
        """è·å–ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        if task_name not in self.task_configs:
            return None
        
        config = self.task_configs[task_name]
        
        if config['feature_method'] == 'rf_importance':
            # ä½¿ç”¨éšæœºæ£®æ—åˆ†æç‰¹å¾é‡è¦æ€§
            rf = RandomForestClassifier(n_estimators=100, random_state=42)
            rf.fit(X_train, y_train)
            
            importance_df = pd.DataFrame({
                'feature': X_train.columns,
                'importance': rf.feature_importances_
            }).sort_values('importance', ascending=False)
            
            return importance_df.head(config['n_features'])
        
        elif config['feature_method'] == 'mutual_info':
            # ä½¿ç”¨äº’ä¿¡æ¯åˆ†æç‰¹å¾é‡è¦æ€§
            selector = SelectKBest(mutual_info_classif, k=config['n_features'])
            selector.fit(X_train, y_train)
            
            importance_df = pd.DataFrame({
                'feature': X_train.columns,
                'score': selector.scores_
            }).sort_values('score', ascending=False)
            
            return importance_df.head(config['n_features'])
        
        return None


def optimize_dataset_features(X_train, y_train, X_test, task_name):
    """
    ä¾¿æ·å‡½æ•°ï¼šä¸ºæ•°æ®é›†ä¼˜åŒ–ç‰¹å¾
    
    Args:
        X_train: è®­ç»ƒç‰¹å¾
        y_train: è®­ç»ƒæ ‡ç­¾
        X_test: æµ‹è¯•ç‰¹å¾
        task_name: ä»»åŠ¡åç§°
        
    Returns:
        ä¼˜åŒ–åçš„è®­ç»ƒå’Œæµ‹è¯•ç‰¹å¾
    """
    optimizer = OptimizedFeatureSelector()
    return optimizer.optimize_dataset_features(X_train, y_train, X_test, task_name)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ”¬ æµ‹è¯•ä¼˜åŒ–ç‰¹å¾é€‰æ‹©å™¨...")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    X_train = pd.DataFrame(np.random.randn(100, 50), columns=[f'feature_{i}' for i in range(50)])
    y_train = pd.Series(np.random.choice([0, 1], 100))
    X_test = pd.DataFrame(np.random.randn(30, 50), columns=[f'feature_{i}' for i in range(50)])
    
    optimizer = OptimizedFeatureSelector()
    
    # æµ‹è¯•ä¸¤ä¸ªä»»åŠ¡
    for task in ['rl_class', 'eab_class']:
        print(f"\nğŸ¯ æµ‹è¯•ä»»åŠ¡: {task}")
        X_train_opt, X_test_opt = optimizer.optimize_dataset_features(X_train, y_train, X_test, task)
        print(f"   åŸå§‹ç‰¹å¾: {X_train.shape[1]} â†’ ä¼˜åŒ–å: {X_train_opt.shape[1]}")
    
    print("\nâœ… ä¼˜åŒ–ç‰¹å¾é€‰æ‹©å™¨æµ‹è¯•å®Œæˆï¼") 