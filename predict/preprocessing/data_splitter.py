"""
æ•°æ®åˆ†å‰²å’Œé¢„å¤„ç†æ¨¡å—
åŠŸèƒ½ï¼šåˆ†å‰²æ•°æ®é›†ï¼Œè¿›è¡Œæ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–
"""

import pandas as pd
import numpy as np
import os
from typing import Dict, Tuple, Optional, List
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.impute import SimpleImputer
import joblib
import warnings
warnings.filterwarnings('ignore')


class DataSplitter:
    def __init__(self, test_size: float = 0.3, random_state: int = 42):
        """
        åˆå§‹åŒ–æ•°æ®åˆ†å‰²å™¨
        
        Args:
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            random_state: éšæœºç§å­
        """
        self.test_size = test_size
        self.random_state = random_state
        self.scalers = {}
        self.imputers = {}
        
    def prepare_datasets(self, df: pd.DataFrame) -> Dict[str, Dict[str, pd.DataFrame]]:
        """
        å‡†å¤‡æ•°æ®é›†ï¼Œè¿›è¡Œé¢„å¤„ç†ä½†ä¸åˆ†å‰²
        
        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            
        Returns:
            é¢„å¤„ç†åçš„å®Œæ•´æ•°æ®é›†å­—å…¸
        """
        datasets = {}
        
        # æ£€æŸ¥å¯ç”¨çš„ä»»åŠ¡
        available_tasks = []
        if 'rl_class' in df.columns:
            available_tasks.append('rl_class')
        if 'eab_class' in df.columns:
            available_tasks.append('eab_class')
        
        if not available_tasks:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„åˆ†ç±»ä»»åŠ¡")
            return datasets
        
        print(f"ğŸ“‹ å‘ç°åˆ†ç±»ä»»åŠ¡: {available_tasks}")
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡å‡†å¤‡æ•°æ®é›†
        for task in available_tasks:
            print(f"\nğŸ¯ å‡†å¤‡ä»»åŠ¡: {task}")
            
            # ç­›é€‰æœ‰æ•ˆæ•°æ®
            task_df = df[df[task].notna()].copy()
            
            if len(task_df) < 10:
                print(f"   âš ï¸ {task} æ•°æ®é‡å¤ªå°‘ ({len(task_df)} æ¡)ï¼Œè·³è¿‡")
                continue
            
            # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
            label_columns = ['rl_class', 'eab_class', 'rl_value', 'eab_value']
            feature_columns = [col for col in task_df.columns 
                             if col not in label_columns + ['record_designation', 'doi', 'formula', 
                                                           'original_formula', 'components', 'main_component', 
                                                           'secondary_component', 'elemental_composition']]
            
            X = task_df[feature_columns].copy()
            y = task_df[task].copy()
            
            print(f"   ğŸ“Š åŸå§‹æ•°æ®: {len(X)} æ ·æœ¬, {len(feature_columns)} ç‰¹å¾")
            print(f"   ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {y.value_counts().to_dict()}")
            
            # æ•°æ®æ¸…ç†å’Œé¢„å¤„ç†
            X_processed, feature_names = self._preprocess_features(X, task)
            
            if X_processed is None or len(X_processed.columns) == 0:
                print(f"   âŒ {task} ç‰¹å¾é¢„å¤„ç†å¤±è´¥ï¼Œè·³è¿‡")
                continue
            
            # å­˜å‚¨å®Œæ•´çš„é¢„å¤„ç†åæ•°æ®é›†ï¼ˆä¸åˆ†å‰²ï¼‰
            datasets[task] = {
                'X': X_processed,  # å®Œæ•´çš„ç‰¹å¾æ•°æ®
                'y': y,           # å®Œæ•´çš„æ ‡ç­¾æ•°æ®
                'feature_names': feature_names
            }
            
            print(f"   âœ… {task} æ•°æ®é›†å‡†å¤‡å®Œæˆ:")
            print(f"      - æ€»æ ·æœ¬æ•°: {len(X_processed)}")
            print(f"      - ç‰¹å¾æ•°: {len(X_processed.columns)}")
            print(f"      - ç‰¹å¾èŒƒå›´: [{X_processed.min().min():.3f}, {X_processed.max().max():.3f}]")
        
        return datasets
    
    def split_data(self, df: pd.DataFrame, target_name: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """
        Split data for a specific target
        
        Args:
            df: Input dataframe
            target_name: Target column name (e.g., 'rl_class', 'eab_class')
            
        Returns:
            X_train, X_test, y_train, y_test
        """
        # Prepare features (exclude target columns and non-numeric columns)
        exclude_columns = ['rl_class', 'eab_class', 'doi', 'record_designation', 
                          'chemical_formula', 'formula', 'original_formula', 'main_component', 'components']
        
        feature_columns = [col for col in df.columns if col not in exclude_columns]
        X = df[feature_columns]
        y = df[target_name]
        
        # Convert boolean columns to numeric
        for col in X.columns:
            if X[col].dtype == bool:
                X[col] = X[col].astype(int)
        
        # Handle any remaining non-numeric columns
        X = X.select_dtypes(include=[np.number])
        
        print(f"   âœ… Features for training: {X.shape[1]} numeric features")
        print(f"   âœ… Target distribution: {y.value_counts().to_dict()}")
        
        # Split the data
        return self.split_dataset(X, y)
    
    def split_dataset(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """
        åˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: æ ‡ç­¾æ•°æ®
            
        Returns:
            è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç‰¹å¾å’Œæ ‡ç­¾
        """
        try:
            # å°è¯•åˆ†å±‚æŠ½æ ·
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, 
                test_size=self.test_size, 
                random_state=self.random_state,
                stratify=y
            )
            print(f"   âœ… ä½¿ç”¨åˆ†å±‚æŠ½æ ·åˆ†å‰²æ•°æ®é›†")
        except Exception as e:
            print(f"   âš ï¸ åˆ†å±‚æŠ½æ ·å¤±è´¥ï¼Œä½¿ç”¨éšæœºæŠ½æ ·: {e}")
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, 
                test_size=self.test_size, 
                random_state=self.random_state
            )
        
        print(f"   ğŸ“Š æ•°æ®åˆ†å‰²å®Œæˆ:")
        print(f"      - è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"      - æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        
        return X_train, X_test, y_train, y_test
    
    def scale_features(self, X_train: pd.DataFrame, X_test: pd.DataFrame, task: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        æ ‡å‡†åŒ–ç‰¹å¾ï¼ˆä»å·²æœ‰çš„é¢„å¤„ç†å™¨æˆ–é‡æ–°è®­ç»ƒï¼‰
        
        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            X_test: æµ‹è¯•é›†ç‰¹å¾
            task: ä»»åŠ¡åç§°
            
        Returns:
            æ ‡å‡†åŒ–åçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾
        """
        # å¦‚æœå·²æœ‰é¢„è®­ç»ƒçš„scalerï¼Œç›´æ¥ä½¿ç”¨
        if task in self.scalers:
            scaler = self.scalers[task]['scaler']
            scaler_name = self.scalers[task]['scaler_name']
            print(f"   ğŸ”§ ä½¿ç”¨å·²ä¿å­˜çš„{scaler_name}")
        else:
            # é‡æ–°è®­ç»ƒscaler
            print(f"   ğŸ”§ å¼€å§‹ç‰¹å¾æ ‡å‡†åŒ–...")
            
            # æ£€æŸ¥æ•°æ®åˆ†å¸ƒï¼Œé€‰æ‹©åˆé€‚çš„æ ‡å‡†åŒ–æ–¹æ³•
            skewness = X_train.skew().abs()
            high_skew_features = skewness[skewness > 2].index.tolist()
            
            print(f"   ğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ:")
            print(f"      é«˜ååº¦ç‰¹å¾ (|skew| > 2): {len(high_skew_features)} ä¸ª")
            
            # æ ¹æ®æ•°æ®ç‰¹ç‚¹é€‰æ‹©æ ‡å‡†åŒ–æ–¹æ³•
            if len(high_skew_features) > len(X_train.columns) * 0.3:
                scaler = RobustScaler()
                scaler_name = "RobustScaler"
                print(f"   ğŸ”§ é€‰æ‹© RobustScaler (é€‚åˆæœ‰å¼‚å¸¸å€¼çš„æ•°æ®)")
            else:
                scaler = StandardScaler()
                scaler_name = "StandardScaler"
                print(f"   ğŸ”§ é€‰æ‹© StandardScaler (æ ‡å‡†æ­£æ€åˆ†å¸ƒ)")
            
            # ä¿å­˜æ ‡å‡†åŒ–å™¨
            self.scalers[task] = {
                'scaler': scaler,
                'scaler_name': scaler_name
            }
        
        # æ‹Ÿåˆå¹¶è½¬æ¢è®­ç»ƒé›†
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index
        )
        
        # è½¬æ¢æµ‹è¯•é›†
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test),
            columns=X_test.columns,
            index=X_test.index
        )
        
        # éªŒè¯æ ‡å‡†åŒ–æ•ˆæœ
        print(f"   âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ ({scaler_name}):")
        print(f"      è®­ç»ƒé›†å‡å€¼: {X_train_scaled.mean().mean():.6f}")
        print(f"      è®­ç»ƒé›†æ ‡å‡†å·®: {X_train_scaled.std().mean():.6f}")
        print(f"      æµ‹è¯•é›†èŒƒå›´: [{X_test_scaled.min().min():.3f}, {X_test_scaled.max().max():.3f}]")
        
        return X_train_scaled, X_test_scaled
    
    def _preprocess_features(self, X: pd.DataFrame, task: str) -> Tuple[Optional[pd.DataFrame], List[str]]:
        """
        é¢„å¤„ç†ç‰¹å¾
        
        Args:
            X: ç‰¹å¾æ•°æ®æ¡†
            task: ä»»åŠ¡åç§°
            
        Returns:
            å¤„ç†åçš„ç‰¹å¾æ•°æ®æ¡†å’Œç‰¹å¾åç§°åˆ—è¡¨
        """
        print(f"   ğŸ”§ å¼€å§‹ç‰¹å¾é¢„å¤„ç†...")
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        print(f"   ğŸ“Š æ•°æ®ç±»å‹åˆ†å¸ƒ:")
        dtype_counts = X.dtypes.value_counts()
        for dtype, count in dtype_counts.items():
            print(f"      {dtype}: {count} åˆ—")
        
        # é€‰æ‹©æ•°å€¼å‹ç‰¹å¾
        numeric_columns = X.select_dtypes(include=[np.number]).columns.tolist()
        print(f"   ğŸ“Š æ•°å€¼å‹ç‰¹å¾: {len(numeric_columns)} ä¸ª")
        
        if len(numeric_columns) == 0:
            print(f"   âŒ æ²¡æœ‰æ•°å€¼å‹ç‰¹å¾")
            return None, []
        
        X_numeric = X[numeric_columns].copy()
        
        # æ£€æŸ¥å’Œå¤„ç†æ— é™å€¼
        inf_cols = []
        for col in X_numeric.columns:
            if np.isinf(X_numeric[col]).any():
                inf_cols.append(col)
                X_numeric[col] = X_numeric[col].replace([np.inf, -np.inf], np.nan)
        
        if inf_cols:
            print(f"   ğŸ”§ å¤„ç†æ— é™å€¼: {len(inf_cols)} åˆ—")
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_info = X_numeric.isnull().sum()
        missing_cols = missing_info[missing_info > 0]
        
        if len(missing_cols) > 0:
            print(f"   ğŸ”§ å¤„ç†ç¼ºå¤±å€¼: {len(missing_cols)} åˆ—")
            print(f"      ç¼ºå¤±æœ€å¤šçš„5åˆ—: {missing_cols.nlargest(5).to_dict()}")
            
            # ä½¿ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼
            imputer = SimpleImputer(strategy='median')
            X_numeric_filled = pd.DataFrame(
                imputer.fit_transform(X_numeric),
                columns=X_numeric.columns,
                index=X_numeric.index
            )
            self.imputers[task] = imputer
            print(f"   âœ… ç¼ºå¤±å€¼å¡«å……å®Œæˆ")
        else:
            X_numeric_filled = X_numeric.copy()
            print(f"   âœ… æ— ç¼ºå¤±å€¼")
        
        # ç§»é™¤å¸¸æ•°ç‰¹å¾ï¼ˆæ–¹å·®ä¸º0çš„ç‰¹å¾ï¼‰
        constant_features = []
        for col in X_numeric_filled.columns:
            if X_numeric_filled[col].var() == 0:
                constant_features.append(col)
        
        if constant_features:
            print(f"   ğŸ”§ ç§»é™¤å¸¸æ•°ç‰¹å¾: {len(constant_features)} ä¸ª")
            X_numeric_filled = X_numeric_filled.drop(columns=constant_features)
        
        # ç§»é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾
        if len(X_numeric_filled.columns) > 1:
            correlation_threshold = 0.99
            corr_matrix = X_numeric_filled.corr().abs()
            
            # æ‰¾åˆ°é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹
            high_corr_pairs = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    if corr_matrix.iloc[i, j] > correlation_threshold:
                        col_i = corr_matrix.columns[i]
                        col_j = corr_matrix.columns[j]
                        high_corr_pairs.append((col_i, col_j, corr_matrix.iloc[i, j]))
            
            # ç§»é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
            features_to_remove = set()
            for col_i, col_j, corr_val in high_corr_pairs:
                features_to_remove.add(col_j)  # ç§»é™¤ç¬¬äºŒä¸ªç‰¹å¾
            
            if features_to_remove:
                print(f"   ğŸ”§ ç§»é™¤é«˜ç›¸å…³ç‰¹å¾: {len(features_to_remove)} ä¸ª (ç›¸å…³æ€§ > {correlation_threshold})")
                X_numeric_filled = X_numeric_filled.drop(columns=list(features_to_remove))
        
        # æœ€ç»ˆæ£€æŸ¥
        final_features = X_numeric_filled.columns.tolist()
        print(f"   âœ… ç‰¹å¾é¢„å¤„ç†å®Œæˆ: {len(final_features)} ä¸ªç‰¹å¾")
        
        if len(final_features) == 0:
            print(f"   âŒ é¢„å¤„ç†åæ²¡æœ‰æœ‰æ•ˆç‰¹å¾")
            return None, []
        
        return X_numeric_filled, final_features
    
    def save_datasets(self, datasets: Dict, cache_dir: str) -> None:
        """
        ä¿å­˜å®Œæ•´æ•°æ®é›†åˆ°ç¼“å­˜ç›®å½•
        
        Args:
            datasets: æ•°æ®é›†å­—å…¸
            cache_dir: ç¼“å­˜ç›®å½•
        """
        os.makedirs(cache_dir, exist_ok=True)
        
        for task_name, data in datasets.items():
            task_dir = os.path.join(cache_dir, task_name)
            os.makedirs(task_dir, exist_ok=True)
            
            # ä¿å­˜å®Œæ•´æ•°æ®é›†
            data['X'].to_pickle(os.path.join(task_dir, 'X_complete.pkl'))
            data['y'].to_pickle(os.path.join(task_dir, 'y_complete.pkl'))
            
            # ä¿å­˜ç‰¹å¾åç§°
            with open(os.path.join(task_dir, 'feature_names.txt'), 'w', encoding='utf-8') as f:
                f.write('\n'.join(data['feature_names']))
            
            # ä¿å­˜é¢„å¤„ç†å™¨
            if task_name in self.scalers:
                joblib.dump(self.scalers[task_name]['scaler'], 
                           os.path.join(task_dir, 'scaler.pkl'))
            
            if task_name in self.imputers:
                joblib.dump(self.imputers[task_name], 
                           os.path.join(task_dir, 'imputer.pkl'))
        
        print(f"âœ… å®Œæ•´æ•°æ®é›†å·²ç¼“å­˜åˆ°: {cache_dir}")
    
    def load_datasets(self, cache_dir: str) -> Optional[Dict]:
        """
        ä»ç¼“å­˜ç›®å½•åŠ è½½å®Œæ•´æ•°æ®é›†
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            
        Returns:
            å®Œæ•´æ•°æ®é›†å­—å…¸æˆ–None
        """
        if not os.path.exists(cache_dir):
            return None
        
        datasets = {}
        
        for task_name in os.listdir(cache_dir):
            task_dir = os.path.join(cache_dir, task_name)
            if not os.path.isdir(task_dir):
                continue
            
            try:
                # åŠ è½½å®Œæ•´æ•°æ®é›†
                X_complete = pd.read_pickle(os.path.join(task_dir, 'X_complete.pkl'))
                y_complete = pd.read_pickle(os.path.join(task_dir, 'y_complete.pkl'))
                
                # åŠ è½½ç‰¹å¾åç§°
                feature_names_file = os.path.join(task_dir, 'feature_names.txt')
                if os.path.exists(feature_names_file):
                    with open(feature_names_file, 'r', encoding='utf-8') as f:
                        feature_names = [line.strip() for line in f.readlines()]
                else:
                    feature_names = X_complete.columns.tolist()
                
                # åŠ è½½é¢„å¤„ç†å™¨
                scaler_file = os.path.join(task_dir, 'scaler.pkl')
                if os.path.exists(scaler_file):
                    scaler = joblib.load(scaler_file)
                    self.scalers[task_name] = {
                        'scaler': scaler,
                        'scaler_name': type(scaler).__name__
                    }
                
                imputer_file = os.path.join(task_dir, 'imputer.pkl')
                if os.path.exists(imputer_file):
                    self.imputers[task_name] = joblib.load(imputer_file)
                
                datasets[task_name] = {
                    'X': X_complete,
                    'y': y_complete,
                    'feature_names': feature_names
                }
                
                print(f"âœ… åŠ è½½ä»»åŠ¡æ•°æ®é›†: {task_name}")
                print(f"   - æ€»æ ·æœ¬æ•°: {len(X_complete)}, {len(X_complete.columns)} ç‰¹å¾")
                
            except Exception as e:
                print(f"âŒ åŠ è½½ä»»åŠ¡ {task_name} å¤±è´¥: {e}")
                continue
        
        return datasets if datasets else None
    
    def check_cached_datasets(self, cache_dir: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼“å­˜çš„å®Œæ•´æ•°æ®é›†
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            
        Returns:
            æ˜¯å¦å­˜åœ¨ç¼“å­˜
        """
        if not os.path.exists(cache_dir):
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ä»»åŠ¡ç›®å½•
        for item in os.listdir(cache_dir):
            task_dir = os.path.join(cache_dir, item)
            if os.path.isdir(task_dir):
                required_files = ['X_complete.pkl', 'y_complete.pkl']
                if all(os.path.exists(os.path.join(task_dir, f)) for f in required_files):
                    return True
        
        return False 