"""
æ•°æ®è´¨é‡ç­›é€‰æ¼”ç¤ºè„šæœ¬
å¿«é€Ÿæµ‹è¯•ç­›é€‰åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import pandas as pd
import numpy as np
from data_quality_filter import DataQualityFilter
from data_splitter import DataSplitter
import os


def demo_data_filtering():
    """æ¼”ç¤ºæ•°æ®è´¨é‡ç­›é€‰åŠŸèƒ½"""
    print("ğŸ”¬ æ•°æ®è´¨é‡ç­›é€‰åŠŸèƒ½æ¼”ç¤º")
    print("=" * 50)
    
    # é€‰æ‹©ä¸€ä¸ªæ•°æ®é›†è¿›è¡Œæ¼”ç¤º
    dataset_name = 'rl_class'  # æˆ–è€… 'eab_class'
    dataset_path = f"datasets/{dataset_name}"
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    if not os.path.exists(dataset_path):
        print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
        print("è¯·ç¡®ä¿ä½ å·²ç»è¿è¡Œäº†æ•°æ®é¢„å¤„ç†æ­¥éª¤ç”Ÿæˆæ•°æ®é›†")
        return
    
    try:
        print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {dataset_name}")
        # åŠ è½½æ•°æ®
        X_complete = pd.read_pickle(f"{dataset_path}/X_complete.pkl")
        y_complete = pd.read_pickle(f"{dataset_path}/y_complete.pkl")
        
        print(f"   âœ… åŸå§‹æ•°æ®: {X_complete.shape[0]} æ ·æœ¬, {X_complete.shape[1]} ç‰¹å¾")
        print(f"   ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {pd.Series(y_complete).value_counts().to_dict()}")
        
        # æ•°æ®åˆ†å‰²
        print(f"\nğŸ”§ åˆ†å‰²æ•°æ®...")
        splitter = DataSplitter(test_size=0.3, random_state=42)
        X_train, X_test, y_train, y_test = splitter.split_dataset(X_complete, y_complete)
        
        print(f"   è®­ç»ƒé›†: {X_train.shape}")
        print(f"   æµ‹è¯•é›†: {X_test.shape}")
        
        # åˆ›å»ºæ•°æ®è´¨é‡ç­›é€‰å™¨
        print(f"\nğŸ” åˆ›å»ºæ•°æ®è´¨é‡ç­›é€‰å™¨...")
        quality_filter = DataQualityFilter(
            cv_folds=3,  # ä½¿ç”¨è¾ƒå°‘çš„foldä»¥åŠ å¿«æ¼”ç¤º
            n_components=5,
            random_state=42
        )
        
        # è¯„ä¼°æ ·æœ¬è´¨é‡
        print(f"\nğŸ“Š è¯„ä¼°è®­ç»ƒé›†æ ·æœ¬è´¨é‡...")
        quality_filter.fit(X_train, y_train, use_feature_selection=True, top_features_ratio=0.3)
        
        # æŸ¥çœ‹è´¨é‡åˆ†æ•°
        sample_scores = quality_filter.get_sample_scores()
        print(f"   âœ… è´¨é‡è¯„ä¼°å®Œæˆ")
        print(f"   ğŸ“ˆ è´¨é‡åˆ†æ•°ç»Ÿè®¡:")
        print(f"      å¹³å‡å€¼: {sample_scores['quality_score'].mean():.4f}")
        print(f"      æ ‡å‡†å·®: {sample_scores['quality_score'].std():.4f}")
        print(f"      æœ€å°å€¼: {sample_scores['quality_score'].min():.4f}")
        print(f"      æœ€å¤§å€¼: {sample_scores['quality_score'].max():.4f}")
        
        # æ˜¾ç¤ºè´¨é‡åˆ†å¸ƒ
        quality_bins = pd.cut(sample_scores['quality_score'], bins=5, labels=['å¾ˆä½', 'ä½', 'ä¸­', 'é«˜', 'å¾ˆé«˜'])
        quality_dist = quality_bins.value_counts()
        print(f"   ğŸ“Š è´¨é‡åˆ†å¸ƒ:")
        for level, count in quality_dist.items():
            print(f"      {level}: {count} æ ·æœ¬ ({count/len(sample_scores)*100:.1f}%)")
        
        # ç­›é€‰é«˜è´¨é‡æ ·æœ¬
        print(f"\nğŸ¯ ç­›é€‰é«˜è´¨é‡æ ·æœ¬...")
        keep_ratio = 0.8  # ä¿ç•™80%
        min_samples_per_class = 5
        
        X_train_filtered, y_train_filtered, filtered_indices = quality_filter.filter_samples(
            X_train, y_train,
            keep_ratio=keep_ratio,
            min_samples_per_class=min_samples_per_class
        )
        
        # æ˜¾ç¤ºç­›é€‰ç»“æœ
        print(f"\nğŸ“ˆ ç­›é€‰ç»“æœ:")
        print(f"   åŸå§‹è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
        print(f"   ç­›é€‰å: {X_train_filtered.shape[0]} æ ·æœ¬ ({X_train_filtered.shape[0]/X_train.shape[0]*100:.1f}%)")
        
        # å¯¹æ¯”ç±»åˆ«åˆ†å¸ƒ
        print(f"\nğŸ“Š ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”:")
        original_dist = pd.Series(y_train).value_counts().sort_index()
        filtered_dist = pd.Series(y_train_filtered).value_counts().sort_index()
        
        for label in original_dist.index:
            orig_count = original_dist.get(label, 0)
            filt_count = filtered_dist.get(label, 0)
            retention_rate = filt_count / orig_count * 100 if orig_count > 0 else 0
            print(f"   ç±»åˆ« {label}: {orig_count} â†’ {filt_count} ({retention_rate:.1f}%)")
        
        # ä¿å­˜è´¨é‡æŠ¥å‘Š
        print(f"\nğŸ’¾ ä¿å­˜è´¨é‡è¯„ä¼°æŠ¥å‘Š...")
        quality_filter.save_quality_report(f"demo_quality_report_{dataset_name}.json", dataset_name)
        
        # è®¡ç®—ç­›é€‰åçš„å¹³å‡è´¨é‡
        filtered_quality_scores = sample_scores.loc[sample_scores['sample_idx'].isin(filtered_indices), 'quality_score']
        print(f"\nğŸ“ˆ è´¨é‡æå‡æ•ˆæœ:")
        print(f"   ç­›é€‰å‰å¹³å‡è´¨é‡: {sample_scores['quality_score'].mean():.4f}")
        print(f"   ç­›é€‰åå¹³å‡è´¨é‡: {filtered_quality_scores.mean():.4f}")
        print(f"   è´¨é‡æå‡: {filtered_quality_scores.mean() - sample_scores['quality_score'].mean():+.4f}")
        
        print(f"\nâœ… æ¼”ç¤ºå®Œæˆï¼")
        print(f"ğŸ“„ æŸ¥çœ‹ç”Ÿæˆçš„JSONæ–‡ä»¶äº†è§£è¯¦ç»†è´¨é‡è¯„ä¼°ç»“æœ")
        
        return True
        
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶")
        print(f"   æœŸæœ›æ–‡ä»¶: {dataset_path}/X_complete.pkl, {dataset_path}/y_complete.pkl")
        print("   è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†æ­¥éª¤ç”Ÿæˆè¿™äº›æ–‡ä»¶")
        return False
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    demo_data_filtering() 