"""
RAGæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
å®ç°åŸºäºå‘é‡æ•°æ®åº“çš„æ–‡æ¡£æ£€ç´¢å’Œæ¨è
"""

import os
import sys
import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import json
import pickle

from .data_loader import DataLoader
from .material_validator import MaterialValidator

logger = logging.getLogger(__name__)


class RAGSystem:
    """RAGæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ - ä½¿ç”¨FAISSæ›¿ä»£ChromaDB"""
    
    def __init__(self, 
                 embedding_model: str = "all-MiniLM-L6-v2",
                 persist_directory: str = None):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            persist_directory: æ•°æ®åº“æŒä¹…åŒ–ç›®å½•
        """
        if persist_directory is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            persist_directory = os.path.join(current_dir, "..", "data", "vectordb")
        
        self.persist_directory = persist_directory
        os.makedirs(persist_directory, exist_ok=True)
        
        # å»¶è¿Ÿåˆå§‹åŒ–åµŒå…¥æ¨¡å‹å’ŒFAISSï¼Œé¿å…å¯åŠ¨æ—¶é˜»å¡
        self.embedding_model_name = embedding_model
        self.embedding_model = None
        self.faiss_index = None
        self.documents = []  # å­˜å‚¨æ–‡æ¡£å†…å®¹
        self.metadatas = []  # å­˜å‚¨å…ƒæ•°æ®
        self.doc_ids = []    # å­˜å‚¨æ–‡æ¡£ID
        self.collection_name = "mxene_materials"
        
        # FAISSç›¸å…³æ–‡ä»¶è·¯å¾„
        self.index_file = os.path.join(persist_directory, "faiss_index.bin")
        self.documents_file = os.path.join(persist_directory, "documents.pkl")
        self.metadatas_file = os.path.join(persist_directory, "metadatas.pkl")
        self.doc_ids_file = os.path.join(persist_directory, "doc_ids.pkl")
        
        self.data_loader = DataLoader()
        
        # å»¶è¿Ÿåˆå§‹åŒ–MaterialValidatorï¼Œé¿å…å¯åŠ¨æ—¶å¼‚å¸¸
        self.material_validator = None
        self._init_material_validator()
        
        self.is_initialized = False
    
    def _init_material_validator(self):
        """å®‰å…¨åˆå§‹åŒ–MaterialValidator"""
        try:
            from .material_validator import MaterialValidator
            self.material_validator = MaterialValidator(self.data_loader)
            logger.info("âœ… MaterialValidatoråˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ MaterialValidatoråˆå§‹åŒ–å¤±è´¥: {e}")
            logger.warning("   é¢„æµ‹åŠŸèƒ½å°†ä¸å¯ç”¨ï¼Œä½†æ¨èåŠŸèƒ½æ­£å¸¸")
            self.material_validator = None
    
    def _init_embedding_and_faiss(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å’ŒFAISS"""
        if self.embedding_model is None:
            logger.info("æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
            try:
                # è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œé¿å…è”ç½‘ä¸‹è½½
                import os
                os.environ['TRANSFORMERS_OFFLINE'] = '1'
                os.environ['HF_DATASETS_OFFLINE'] = '1'
                
                # å°è¯•åŠ è½½æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹
                logger.info(f"å°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹: {self.embedding_model_name}")
                self.embedding_model = SentenceTransformer(self.embedding_model_name, 
                                                         local_files_only=True)
                logger.info(f"âœ… æˆåŠŸåŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹: {self.embedding_model_name}")
                
            except Exception as local_error:
                logger.warning(f"âš ï¸ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {local_error}")
                logger.info("å°è¯•åœ¨çº¿ä¸‹è½½æ¨¡å‹...")
                
                try:
                    # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œå°è¯•ä¸‹è½½ï¼ˆä»…é¦–æ¬¡ï¼‰
                    self.embedding_model = SentenceTransformer(self.embedding_model_name)
                    logger.info(f"âœ… æˆåŠŸä¸‹è½½å¹¶åŠ è½½åµŒå…¥æ¨¡å‹: {self.embedding_model_name}")
                    logger.info("ğŸ’¡ ä¸‹æ¬¡å¯åŠ¨å°†ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œæ— éœ€è”ç½‘")
                    
                except Exception as download_error:
                    logger.error(f"âŒ æ¨¡å‹ä¸‹è½½ä¹Ÿå¤±è´¥: {download_error}")
                    logger.error("ğŸ”§ è§£å†³æ–¹æ¡ˆ:")
                    logger.error("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
                    logger.error("   2. æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° ~/.cache/huggingface/transformers/")
                    logger.error("   3. æˆ–ä½¿ç”¨å…¶ä»–å¯ç”¨çš„æœ¬åœ°æ¨¡å‹")
                    raise Exception("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ¨¡å‹ç¼“å­˜")
        
        if self.faiss_index is None:
            logger.info("æ­£åœ¨åˆå§‹åŒ–FAISSç´¢å¼•...")
            try:
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²ä¿å­˜çš„ç´¢å¼•
                if os.path.exists(self.index_file):
                    logger.info("åŠ è½½å·²å­˜åœ¨çš„FAISSç´¢å¼•...")
                    self.faiss_index = faiss.read_index(self.index_file)
                    
                    # åŠ è½½å¯¹åº”çš„æ–‡æ¡£æ•°æ®
                    with open(self.documents_file, 'rb') as f:
                        self.documents = pickle.load(f)
                    with open(self.metadatas_file, 'rb') as f:
                        self.metadatas = pickle.load(f)
                    with open(self.doc_ids_file, 'rb') as f:
                        self.doc_ids = pickle.load(f)
                    
                    logger.info(f"âœ… æˆåŠŸåŠ è½½FAISSç´¢å¼•ï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£")
                else:
                    # åˆ›å»ºæ–°çš„ç©ºç´¢å¼•
                    dimension = 384  # all-MiniLM-L6-v2çš„å‘é‡ç»´åº¦
                    self.faiss_index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ä½œä¸ºç›¸ä¼¼åº¦
                    logger.info("âœ… åˆ›å»ºæ–°çš„FAISSç´¢å¼•")
                
            except Exception as e:
                logger.error(f"âŒ FAISSåˆå§‹åŒ–å¤±è´¥: {e}")
                raise

    def _has_any_docs(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²æœ‰æ–‡æ¡£"""
        return len(self.documents) > 0

    def _encode_texts(self, texts: List[str]) -> np.ndarray:
        """ç»Ÿä¸€çš„æ–‡æœ¬ç¼–ç  + æ¸…æ´—"""
        vecs = self.embedding_model.encode(texts, convert_to_numpy=True)
        vecs = np.nan_to_num(vecs, nan=0.0, posinf=0.0, neginf=0.0).astype("float32")
        # å½’ä¸€åŒ–å‘é‡ï¼Œç”¨äºå†…ç§¯è®¡ç®—ç›¸ä¼¼åº¦
        faiss.normalize_L2(vecs)
        return vecs

    def initialize_database(self, force_rebuild: bool = False, quick_mode: bool = False) -> bool:
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        
        Args:
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºæ•°æ®åº“
            quick_mode: å¿«é€Ÿæ¨¡å¼ï¼Œè·³è¿‡è€—æ—¶çš„å‘é‡åŒ–è¿‡ç¨‹
            
        Returns:
            æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        """
        try:
            logger.info(f"å¼€å§‹åˆå§‹åŒ–æ•°æ®åº“... (å¿«é€Ÿæ¨¡å¼: {quick_mode})")
            
            # å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡å‘é‡æ•°æ®åº“ï¼Œç›´æ¥è¿”å›æˆåŠŸ
            if quick_mode:
                logger.info("å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡å‘é‡æ•°æ®åº“åˆå§‹åŒ–")
                self.is_initialized = True
                return True
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å’ŒFAISSï¼ˆå¯èƒ½è€—æ—¶ï¼‰
            logger.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶...")
            self._init_embedding_and_faiss()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åŠ è½½æ•°æ®
            if not force_rebuild:
                if self._has_any_docs():
                    logger.info("æ•°æ®åº“ä¸­å·²æœ‰æ•°æ®ï¼Œè·³è¿‡æ•°æ®åŠ è½½")
                    self.is_initialized = True
                    return True
                else:
                    logger.info("æ•°æ®åº“ä¸ºç©ºï¼Œéœ€è¦åŠ è½½æ•°æ®")
            
            # åŠ è½½æ•°æ®
            logger.info("å¼€å§‹åŠ è½½MXeneæ•°æ®...")
            mxene_data = self.data_loader.get_mxene_data()
            if not mxene_data:
                logger.warning("æœªæ‰¾åˆ°MXeneæ•°æ®ï¼Œä½†æ ‡è®°ä¸ºå·²åˆå§‹åŒ–")
                self.is_initialized = True
                return True
            logger.info(f"åŠ è½½äº† {len(mxene_data)} æ¡MXeneæ•°æ®")
            
            # å¦‚éœ€é‡å»ºï¼Œå…ˆæ¸…ç©ºç°æœ‰æ•°æ®
            if force_rebuild and self._has_any_docs():
                logger.info("å¼ºåˆ¶é‡å»ºï¼Œæ¸…ç©ºç°æœ‰æ•°æ®...")
                self._clear_index()
            
            # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºä¸“é—¨çš„æ–‡æ¡£
            all_documents: List[str] = []
            all_metadatas: List[Dict[str, Any]] = []
            all_ids: List[str] = []
            
            categories = ['chemical_formula', 'synthesis_method', 'testing_procedure']
            for category in categories:
                docs, metas, ids = self._create_category_documents(mxene_data, category)
                all_documents.extend(docs)
                all_metadatas.extend(metas)
                all_ids.extend(ids)
                logger.info(f"ä¸ºç±»åˆ« {category} åˆ›å»ºäº† {len(docs)} ä¸ªæ–‡æ¡£")
            
            # æ‰¹é‡æ·»åŠ æ–‡æ¡£åˆ°FAISSç´¢å¼•
            if all_documents:
                batch_size = 50  # å‡å°æ‰¹æ¬¡å¤§å°ï¼Œé¿å…è¶…æ—¶
                total_batches = (len(all_documents) - 1) // batch_size + 1
                logger.info(f"å¼€å§‹åˆ†æ‰¹æ·»åŠ  {len(all_documents)} ä¸ªæ–‡æ¡£ï¼Œå…± {total_batches} æ‰¹")
                
                try:
                    for i in range(0, len(all_documents), batch_size):
                        batch_docs = all_documents[i:i+batch_size]
                        batch_metas = all_metadatas[i:i+batch_size]
                        batch_ids = all_ids[i:i+batch_size]
                        
                        batch_num = i // batch_size + 1
                        logger.info(f"æ­£åœ¨æ·»åŠ æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_docs)} ä¸ªæ–‡æ¡£)")
                        
                        try:
                            # è¿‡æ»¤ç©ºæ–‡æœ¬
                            clean = [(d, m, _id) for (d, m, _id) in zip(batch_docs, batch_metas, batch_ids) if d and d.strip()]
                            if not clean:
                                logger.warning(f"æ‰¹æ¬¡ {batch_num} å…¨æ˜¯ç©ºæ–‡æ¡£ï¼Œè·³è¿‡")
                                continue
                            docs2, metas2, ids2 = zip(*clean)

                            vecs = self._encode_texts(list(docs2))
                            self._add_vectors_to_index(vecs, list(docs2), list(metas2), list(ids2))
                            logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} æ·»åŠ æˆåŠŸ")
                        except Exception as batch_error:
                            logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} æ·»åŠ å¤±è´¥: {batch_error}")
                            # å°è¯•é€æ¡æ·»åŠ 
                            if len(batch_docs) > 1:
                                logger.info("å°è¯•å•ä¸ªæ–‡æ¡£æ·»åŠ ...")
                                for j, (doc, meta, doc_id) in enumerate(zip(batch_docs, batch_metas, batch_ids)):
                                    try:
                                        if not doc or not doc.strip():
                                            continue
                                        _v = self._encode_texts([doc])
                                        self._add_vectors_to_index(_v, [doc], [meta], [doc_id])
                                    except Exception as single_error:
                                        logger.warning(f"è·³è¿‡æ–‡æ¡£ {doc_id}: {single_error}")
                            else:
                                logger.warning(f"è·³è¿‡æ‰¹æ¬¡ {batch_num}")
                                continue
                    logger.info("âœ… æ–‡æ¡£æ·»åŠ æµç¨‹å®Œæˆ")
                    
                    # ä¿å­˜ç´¢å¼•å’Œæ•°æ®
                    self._save_index()
                    
                except Exception as add_error:
                    logger.error(f"âŒ æ·»åŠ æ–‡æ¡£è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {add_error}")
                    logger.warning("âš ï¸ å°†æ ‡è®°ä¸ºå·²åˆå§‹åŒ–ä»¥ä¿æŒç³»ç»Ÿå¯ç”¨")
            else:
                logger.warning("æ²¡æœ‰æ–‡æ¡£å¯æ·»åŠ åˆ°æ•°æ®åº“")
            
            self.is_initialized = True
            logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def _clear_index(self):
        """æ¸…ç©ºFAISSç´¢å¼•å’Œç›¸å…³æ•°æ®"""
        dimension = 384  # all-MiniLM-L6-v2çš„å‘é‡ç»´åº¦
        self.faiss_index = faiss.IndexFlatIP(dimension)
        self.documents = []
        self.metadatas = []
        self.doc_ids = []
    
    def _add_vectors_to_index(self, vectors: np.ndarray, documents: List[str], 
                             metadatas: List[Dict[str, Any]], ids: List[str]):
        """æ·»åŠ å‘é‡åˆ°FAISSç´¢å¼•"""
        # æ·»åŠ å‘é‡åˆ°FAISSç´¢å¼•
        self.faiss_index.add(vectors)
        
        # åŒæ­¥æ›´æ–°æ–‡æ¡£æ•°æ®
        self.documents.extend(documents)
        self.metadatas.extend(metadatas)
        self.doc_ids.extend(ids)
    
    def _save_index(self):
        """ä¿å­˜FAISSç´¢å¼•å’Œç›¸å…³æ•°æ®"""
        try:
            # ä¿å­˜FAISSç´¢å¼•
            faiss.write_index(self.faiss_index, self.index_file)
            
            # ä¿å­˜æ–‡æ¡£æ•°æ®
            with open(self.documents_file, 'wb') as f:
                pickle.dump(self.documents, f)
            with open(self.metadatas_file, 'wb') as f:
                pickle.dump(self.metadatas, f)
            with open(self.doc_ids_file, 'wb') as f:
                pickle.dump(self.doc_ids, f)
                
            logger.info("âœ… FAISSç´¢å¼•å’Œæ•°æ®å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜FAISSç´¢å¼•å¤±è´¥: {e}")
    
    def _build_document_text(self, item: Dict[str, Any]) -> str:
        """
        æ„å»ºæ–‡æ¡£æ–‡æœ¬
        
        Args:
            item: æ•°æ®é¡¹
            
        Returns:
            ç»„åˆåçš„æ–‡æ¡£æ–‡æœ¬
        """
        parts = []
        
        if item.get('title'):
            parts.append(f"æ ‡é¢˜: {item['title']}")
        
        if item.get('abstract'):
            parts.append(f"æ‘˜è¦: {item['abstract']}")
        
        if item.get('chemical_formula'):
            parts.append(f"åŒ–å­¦å¼: {item['chemical_formula']}")
        
        if item.get('synthesis_method'):
            parts.append(f"åˆæˆæ–¹æ³•: {item['synthesis_method']}")
        
        if item.get('testing_procedure'):
            parts.append(f"æµ‹è¯•æµç¨‹: {item['testing_procedure']}")
        
        # æ·»åŠ å†…å®¹æ–‡æœ¬ï¼ˆæˆªå–å‰1000å­—ç¬¦é¿å…è¿‡é•¿ï¼‰
        content = item.get('content', [])
        if isinstance(content, list):
            content_text = ' '.join(content)[:1000]
        else:
            content_text = str(content)[:1000]
        
        if content_text.strip():
            parts.append(f"å†…å®¹: {content_text}")
        
        return '\n'.join(parts)
    
    def _create_category_documents(self, data: List[Dict[str, Any]], 
                                 category: str) -> Tuple[List[str], List[Dict], List[str]]:
        """
        ä¸ºç‰¹å®šç±»åˆ«åˆ›å»ºä¸“é—¨çš„æ–‡æ¡£
        
        Args:
            data: æ•°æ®åˆ—è¡¨
            category: ç±»åˆ«åç§°
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨ã€å…ƒæ•°æ®åˆ—è¡¨ã€IDåˆ—è¡¨
        """
        documents = []
        metadatas = []
        ids = []
        
        category_map = {
            'chemical_formula': 'åŒ–å­¦å¼',
            'synthesis_method': 'åˆæˆæ–¹æ³•',
            'testing_procedure': 'æµ‹è¯•æµç¨‹'
        }
        
        # å…³é”®è¯ç”¨äºä»å†…å®¹ä¸­è¯†åˆ«ç›¸å…³ä¿¡æ¯
        category_keywords = {
            'chemical_formula': ['Ti3C2', 'MXene', 'formula', 'composition', 'chemical', 'Ti2C', 'Nb2C', 'V2C', 'Ta2C', 'Mo2C'],
            'synthesis_method': ['synthesis', 'preparation', 'fabrication', 'etching', 'HF', 'method', 'procedure', 'process'],
            'testing_procedure': ['characterization', 'measurement', 'analysis', 'test', 'XRD', 'SEM', 'TEM', 'VNA', 'electromagnetic']
        }
        
        keywords = category_keywords.get(category, [])
        
        for i, item in enumerate(data):
            # é¦–å…ˆæ£€æŸ¥åŸå§‹å­—æ®µ
            value = item.get(category, '')
            if not value or not value.strip():
                # å¦‚æœåŸå§‹å­—æ®µä¸ºç©ºï¼Œä»titleã€abstractã€contentä¸­æŸ¥æ‰¾ç›¸å…³å†…å®¹
                content_text = ""
                if item.get('title'):
                    content_text += item['title'] + " "
                if item.get('abstract'):
                    content_text += item['abstract'] + " "
                if item.get('content'):
                    if isinstance(item['content'], list):
                        content_text += " ".join(item['content'])
                    else:
                        content_text += str(item['content'])
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸å…³å…³é”®è¯
                content_lower = content_text.lower()
                if any(keyword.lower() in content_lower for keyword in keywords):
                    # æå–ç›¸å…³ç‰‡æ®µä½œä¸ºå€¼
                    sentences = content_text.split('.')
                    relevant_sentences = []
                    for sentence in sentences:
                        if any(keyword.lower() in sentence.lower() for keyword in keywords):
                            relevant_sentences.append(sentence.strip())
                    
                    if relevant_sentences:
                        value = '. '.join(relevant_sentences[:3])  # å–å‰3ä¸ªç›¸å…³å¥å­
            
            if value and value.strip():
                # åˆ›å»ºä¸“é—¨é’ˆå¯¹è¯¥ç±»åˆ«çš„æ–‡æ¡£
                doc_text = f"{category_map[category]}: {value}\n"
                
                # æ·»åŠ ç›¸å…³ä¸Šä¸‹æ–‡
                if item.get('title'):
                    doc_text += f"ç ”ç©¶æ ‡é¢˜: {item['title']}\n"
                if item.get('abstract'):
                    doc_text += f"ç ”ç©¶æ‘˜è¦: {item['abstract'][:300]}...\n"
                
                documents.append(doc_text)
                
                metadata = {
                    'doi': item.get('doi', ''),
                    'title': item.get('title', ''),
                    'publisher': item.get('publisher', ''),
                    'source': item.get('source', ''),
                    'type': category,
                    'value': value
                }
                metadatas.append(metadata)
                ids.append(f"{category}_{i}")
        
        return documents, metadatas, ids
    
    def search_similar(self, query: str, n_results: int = 5, 
                      category: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸ä¼¼æ–‡æ¡£ï¼ˆä½¿ç”¨æ‰‹åŠ¨ query_embeddingsï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            category: æŒ‡å®šæœç´¢ç±»åˆ«
            
        Returns:
            ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        # ç¡®ä¿ç»„ä»¶å·²åˆå§‹åŒ–
        if self.faiss_index is None:
            logger.warning("æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œå°è¯•åˆå§‹åŒ–...")
            if not self.initialize_database():
                return []
        
        if not self.is_initialized:
            logger.warning("æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œå°è¯•åˆå§‹åŒ–...")
            if not self.initialize_database():
                return []
        
        try:
            # æ‰‹åŠ¨ç¼–ç æŸ¥è¯¢å‘é‡
            qvec = self._encode_texts([query])
            
            # æ‰§è¡Œæœç´¢
            D, I = self.faiss_index.search(qvec, min(n_results * 3, len(self.documents)))  # è·å–æ›´å¤šç»“æœç”¨äºè¿‡æ»¤
            
            search_results = []
            for distance, idx in zip(D[0], I[0]):
                if idx == -1:  # FAISSè¿”å›-1è¡¨ç¤ºæ— æ•ˆç´¢å¼•
                    continue
                    
                doc_idx = int(idx)
                if doc_idx >= len(self.metadatas):
                    continue
                    
                doc_meta = self.metadatas[doc_idx]
                doc_text = self.documents[doc_idx]
                
                # åº”ç”¨ç±»åˆ«è¿‡æ»¤
                if category and doc_meta.get('type') != category:
                    continue
                
                result = {
                    'document': doc_text,
                    'metadata': doc_meta,
                    'similarity_score': float(distance),  # FAISSä½¿ç”¨å†…ç§¯ï¼Œå€¼è¶Šå¤§è¶Šç›¸ä¼¼
                    'distance': float(distance)
                }
                search_results.append(result)
                
                # è¾¾åˆ°æ‰€éœ€ç»“æœæ•°é‡å³åœæ­¢
                if len(search_results) >= n_results:
                    break
            
            return search_results
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def recommend_chemical_formula(self, query: str, n_results: int = 3) -> List[Dict[str, Any]]:
        """
        æ¨èåŒ–å­¦å¼
        """
        enhanced_query = f"MXene åŒ–å­¦å¼ ææ–™ç»„æˆ {query}"
        results = self.search_similar(
            enhanced_query, 
            n_results=n_results * 2,
            category='chemical_formula'
        )
        
        formulas: Dict[str, Dict[str, Any]] = {}
        for result in results:
            meta = result['metadata']
            formula = meta.get('value', '')
            if formula and formula not in formulas:
                formulas[formula] = {
                    'formula': formula,
                    'source': meta.get('title', 'æœªçŸ¥æ¥æº'),
                    'doi': meta.get('doi', ''),
                    'score': result['similarity_score'],
                    'description': (result.get('document') or '')[:200] + '...'
                }
        
        sorted_formulas = sorted(
            formulas.values(), 
            key=lambda x: x['score'], 
            reverse=True
        )[:n_results]
        
        return sorted_formulas
    
    def recommend_synthesis_method(self, query: str, n_results: int = 3) -> List[Dict[str, Any]]:
        """
        æ¨èåˆæˆå·¥è‰º
        """
        enhanced_query = f"MXene åˆæˆæ–¹æ³• åˆ¶å¤‡å·¥è‰º {query}"
        results = self.search_similar(
            enhanced_query,
            n_results=n_results * 2,
            category='synthesis_method'
        )
        
        methods: Dict[str, Dict[str, Any]] = {}
        for result in results:
            meta = result['metadata']
            method = meta.get('value', '')
            if method and method not in methods:
                methods[method] = {
                    'method': method,
                    'source': meta.get('title', 'æœªçŸ¥æ¥æº'),
                    'doi': meta.get('doi', ''),
                    'score': result['similarity_score'],
                    'description': (result.get('document') or '')[:200] + '...'
                }
        
        sorted_methods = sorted(
            methods.values(),
            key=lambda x: x['score'],
            reverse=True
        )[:n_results]
        
        return sorted_methods
    
    def recommend_testing_procedure(self, query: str, n_results: int = 3) -> List[Dict[str, Any]]:
        """
        æ¨èæµ‹è¯•æµç¨‹
        """
        enhanced_query = f"MXene æµ‹è¯•æ–¹æ³• è¡¨å¾æ‰‹æ®µ æ€§èƒ½æµ‹è¯• {query}"
        results = self.search_similar(
            enhanced_query,
            n_results=n_results * 2,
            category='testing_procedure'
        )
        
        procedures: Dict[str, Dict[str, Any]] = {}
        for result in results:
            meta = result['metadata']
            procedure = meta.get('value', '')
            if procedure and procedure not in procedures:
                procedures[procedure] = {
                    'procedure': procedure,
                    'source': meta.get('title', 'æœªçŸ¥æ¥æº'),
                    'doi': meta.get('doi', ''),
                    'score': result['similarity_score'],
                    'description': (result.get('document') or '')[:200] + '...'
                }
        
        sorted_procedures = sorted(
            procedures.values(),
            key=lambda x: x['score'],
            reverse=True
        )[:n_results]
        
        return sorted_procedures
    
    def get_comprehensive_recommendations(self, query: str) -> Dict[str, Any]:
        """
        è·å–ç»¼åˆæ¨èï¼ˆåŒ–å­¦å¼ã€åˆæˆå·¥è‰ºã€æµ‹è¯•æµç¨‹ï¼‰
        """
        recommendations = {
            'chemical_formulas': self.recommend_chemical_formula(query),
            'synthesis_methods': self.recommend_synthesis_method(query),
            'testing_procedures': self.recommend_testing_procedure(query)
        }
        return recommendations
    
    def get_enhanced_recommendations(self, query: str, enable_prediction: bool = True) -> Dict[str, Any]:
        """
        è·å–å¢å¼ºç‰ˆæ¨èï¼ˆåŒ…å«ææ–™éªŒè¯å’Œæ€§èƒ½é¢„æµ‹ï¼‰
        """
        logger.info(f"ğŸš€ è·å–å¢å¼ºç‰ˆæ¨è: {query}")
        
        # 1. è·å–åŸºç¡€æ¨è
        base_recommendations = self.get_comprehensive_recommendations(query)
        
        # 2. éªŒè¯æ¨èçš„åŒ–å­¦å¼
        enhanced_formulas = []
        for formula_item in base_recommendations['chemical_formulas']:
            formula = formula_item['formula']
            
            # éªŒè¯ææ–™æ€§èƒ½ï¼ˆæ ¹æ®å‚æ•°å†³å®šæ˜¯å¦é¢„æµ‹ï¼‰
            if self.material_validator:
                validation_result = self.material_validator.validate_material(formula, enable_prediction=enable_prediction)
            else:
                validation_result = {
                    'formula': formula,
                    'source': 'none',
                    'found_in_db': False,
                    'experimental_data': None,
                    'prediction': None,
                    'confidence': 'low',
                    'summary': "ææ–™éªŒè¯å™¨ä¸å¯ç”¨"
                }
            
            enhanced_item = {
                **formula_item,
                'validation': validation_result,
                'has_experimental_data': validation_result['found_in_db'],
                'performance_summary': self._generate_performance_summary(validation_result)
            }
            enhanced_formulas.append(enhanced_item)
        
        enhanced_recommendations = {
            'chemical_formulas': enhanced_formulas,
            'synthesis_methods': base_recommendations['synthesis_methods'],
            'testing_procedures': base_recommendations['testing_procedures'],
            'validation_summary': self._generate_validation_summary(enhanced_formulas)
        }
        
        logger.info(f"âœ… å®Œæˆå¢å¼ºæ¨èï¼ŒéªŒè¯äº†{len(enhanced_formulas)}ä¸ªåŒ–å­¦å¼")
        return enhanced_recommendations
    
    def get_ai_predictions_for_recommendations(self, query: str) -> List[Dict[str, Any]]:
        """
        ä¸ºæ¨èçš„åŒ–å­¦å¼è·å–AIé¢„æµ‹
        é€»è¾‘ï¼šå¦‚æœæœ‰æ•°æ®åº“æ•°æ®å°±è¿”å›ï¼Œå¦‚æœæ²¡æœ‰å°±è¿›è¡ŒAIé¢„æµ‹
        """
        logger.info(f"ğŸ¤– è·å–AIé¢„æµ‹: {query}")
        
        base_recommendations = self.get_comprehensive_recommendations(query)
        
        prediction_results = []
        for formula_item in base_recommendations['chemical_formulas']:
            formula = formula_item['formula']
            
            if self.material_validator:
                validation_result = self.material_validator.validate_material(formula, enable_prediction=True)
            else:
                validation_result = {
                    'formula': formula,
                    'source': 'none',
                    'found_in_db': False,
                    'experimental_data': None,
                    'prediction': None,
                    'confidence': 'low',
                    'summary': "ææ–™éªŒè¯å™¨ä¸å¯ç”¨"
                }
            
            prediction_item = {
                **formula_item,
                'validation': validation_result,
                'has_experimental_data': validation_result['found_in_db'],
            }
            prediction_results.append(prediction_item)
        
        logger.info(f"âœ… å®ŒæˆAIé¢„æµ‹ï¼Œå¤„ç†äº†{len(prediction_results)}ä¸ªåŒ–å­¦å¼")
        return prediction_results
    
    def _generate_performance_summary(self, validation_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ€§èƒ½æ‘˜è¦"""
        if validation_result['found_in_db']:
            data_count = len(validation_result['experimental_data'])
            return f"å®éªŒæ•°æ® ({data_count}æ¡ç ”ç©¶)"
        elif validation_result['source'] == 'prediction' and validation_result['prediction']:
            pred = validation_result['prediction']
            return f"AIé¢„æµ‹ (ç½®ä¿¡åº¦: {pred['confidence']:.2f})"
        elif validation_result['source'] == 'none':
            return "ä»…æ¨èæ•°æ® (æ— æ€§èƒ½éªŒè¯)"
        else:
            return "æ— æ€§èƒ½æ•°æ®"
    
    def _generate_validation_summary(self, enhanced_formulas: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”ŸæˆéªŒè¯æ‘˜è¦"""
        total = len(enhanced_formulas)
        experimental_count = sum(1 for item in enhanced_formulas if item['has_experimental_data'])
        predicted_count = sum(1 for item in enhanced_formulas if not item['has_experimental_data'] and item['validation']['prediction'])
        
        return {
            'total_formulas': total,
            'experimental_data_count': experimental_count,
            'predicted_count': predicted_count,
            'coverage_rate': experimental_count / total if total > 0 else 0
        }
    
    def clear_database(self):
        """æ¸…ç©ºæ•°æ®åº“"""
        try:
            # æ¸…ç©ºFAISSç´¢å¼•å’Œæ•°æ®
            self._clear_index()
            
            # åˆ é™¤æŒä¹…åŒ–æ–‡ä»¶
            for file_path in [self.index_file, self.documents_file, self.metadatas_file, self.doc_ids_file]:
                if os.path.exists(file_path):
                    os.remove(file_path)
            
            self.is_initialized = False
            logger.info("æ•°æ®åº“å·²æ¸…ç©ºå¹¶é‡å»º")
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºæ•°æ®åº“å¤±è´¥: {e}")
    
    def get_database_stats(self) -> Dict[str, Any]:
        """
        è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯ï¼ˆé¿å…ä½¿ç”¨ count/query_textsï¼Œå…¨éƒ¨ç”¨ get(limit=...)ï¼‰
        """
        try:
            # ä¼°ç®—æ€»é‡ï¼šåˆ†æ‰¹ getï¼Œåªæ‹¿ idï¼ˆé¿å…æ‹‰ embeddings/documentsï¼‰
            total = len(self.documents)

            # æŒ‰ç±»å‹ç»Ÿè®¡ï¼ˆä»…åˆ¤æ–­æ˜¯å¦å­˜åœ¨ï¼‰
            type_counts = {}
            for doc_type in ['general', 'chemical_formula', 'synthesis_method', 'testing_procedure']:
                try:
                    got = sum(1 for meta in self.metadatas if meta.get('type') == doc_type)
                    type_counts[doc_type] = got
                except Exception:
                    type_counts[doc_type] = 0
            
            return {
                'total_documents': total,
                'type_distribution': type_counts,
                'is_initialized': self.is_initialized
            }
            
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                'total_documents': 0,
                'type_distribution': {},
                'is_initialized': False
            }

    def get_direct_recommendations(self, query: str, n_results: int = 10) -> Dict[str, Any]:
        """
        ç›´æ¥æ¨èæµç¨‹ï¼šä¸­æ–‡è½¬è‹±æ–‡ -> è‹±æ–‡æœç´¢æ–‡æ¡£ -> è·å–DOIæ–‡çŒ® -> LLMåˆ†æ -> è¿”å›ç»“æ„åŒ–ç»“æœ
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢ï¼ˆå¦‚"å¼‚è´¨ç»“"ï¼‰
            n_results: æœç´¢ç»“æœæ•°é‡
            
        Returns:
            åŒ…å«åŒ–å­¦å¼ã€åˆæˆå·¥è‰ºã€æµ‹è¯•æµç¨‹çš„å­—å…¸ï¼Œä»¥åŠè‹±æ–‡æŸ¥è¯¢
        """
        logger.info(f"ğŸ” ç›´æ¥æ¨è: {query}")
        
        # æ­¥éª¤1: ä¸­æ–‡è½¬ä¸“ä¸šè‹±æ–‡
        from .llm_handler import LLMHandler
        from .config_cache import config_cache
        
        # åˆ›å»ºLLMå¤„ç†å™¨å¹¶åŠ è½½é…ç½®
        llm_handler = LLMHandler()
        config = config_cache.load_config()
        if config:
            llm_handler.set_api_config(config)
        
        english_query = query  # é»˜è®¤ä½¿ç”¨åŸæŸ¥è¯¢
        if llm_handler.is_api_ready():
            english_query = llm_handler.translate_to_professional_english(query)
            logger.info(f"ğŸŒ è‹±æ–‡æŸ¥è¯¢: {english_query}")
        else:
            logger.warning("LLMæœªé…ç½®ï¼Œè·³è¿‡ç¿»è¯‘æ­¥éª¤")
        
        # æ­¥éª¤2: ä½¿ç”¨è‹±æ–‡æŸ¥è¯¢æœç´¢ç›¸å…³æ–‡æ¡£ï¼Œåªè·å–å‰2ä¸ªæœ€åŒ¹é…çš„DOI
        search_results = self.search_similar(english_query, n_results=n_results)
        
        if not search_results:
            logger.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            return {
                "original_query": query,
                "english_query": english_query,
                "chemical_formulas": [],
                "synthesis_methods": [],
                "testing_procedures": [],
                "error": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
            }
        
        # æ­¥éª¤3: å¯¹æœç´¢ç»“æœè¿›è¡Œå»é‡ï¼Œåªä¿ç•™å‰2ä¸ªæœ€åŒ¹é…çš„ç»“æœï¼Œå¹¶æå–DOI
        unique_results, top_dois = self._get_unique_top_results(search_results, max_results=2)
        logger.info("=" * 60)
        logger.info("ğŸ“„ å»é‡å’ŒDOIæå–è¯¦æƒ…:")
        logger.info("=" * 60)
        logger.info(f"ğŸ” ä» {len(search_results)} ä¸ªæœç´¢ç»“æœä¸­å»é‡åä¿ç•™ {len(unique_results)} ä¸ª")
        logger.info(f"ğŸ“‹ æå–åˆ°çš„DOI: {top_dois}")
        
        # æ˜¾ç¤ºå»é‡åçš„ç»“æœä¿¡æ¯
        for i, result in enumerate(unique_results, 1):
            meta = result.get('metadata', {})
            doi = meta.get('doi', 'N/A')
            score = result.get('similarity_score', 0)
            title = meta.get('title', 'æœªçŸ¥')
            logger.info(f"  å»é‡ç»“æœ{i}: DOI={doi}, ç›¸ä¼¼åº¦={score:.3f}, æ ‡é¢˜={title[:50]}...")
        logger.info("=" * 60)
        
        # æ­¥éª¤4: æ ¹æ®DOIä»all_mxene.jsonä¸­è·å–å®Œæ•´æ–‡çŒ®ä¿¡æ¯
        full_literature_data = self._get_literature_by_dois(top_dois)
        logger.info("ğŸ“š æ–‡çŒ®æ£€ç´¢è¯¦æƒ…:")
        logger.info(f"ğŸ“„ æ£€ç´¢åˆ° {len(full_literature_data)} ç¯‡å®Œæ•´æ–‡çŒ®")
        
        for i, paper in enumerate(full_literature_data, 1):
            title = paper.get('title', 'æœªçŸ¥')
            authors = paper.get('authors', 'æœªçŸ¥')
            year = paper.get('year', 'æœªçŸ¥')
            doi = paper.get('doi', 'æœªçŸ¥')
            logger.info(f"  æ–‡çŒ®{i}: {title[:60]}... (ä½œè€…: {authors[:30]}..., å¹´ä»½: {year}, DOI: {doi})")
        logger.info("=" * 60)
        
        # æ­¥éª¤5: ç»„åˆæœç´¢ç»“æœå’Œæ–‡çŒ®æ•°æ®
        # ç»„åˆRAGæœç´¢ç»“æœï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰å’Œall.jsonæ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
        
        # ç¬¬ä¸€éƒ¨åˆ†ï¼šRAGå‘é‡æœç´¢ç»“æœï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
        context_text = self._format_search_results_for_llm(unique_results)
        literature_text = self._format_literature_for_llm(full_literature_data)
        
        # ç¬¬äºŒéƒ¨åˆ†ï¼šå°è¯•è·å–all.jsonæ ¼å¼çš„ç»“æ„åŒ–æ•°æ®
        json_context = self._format_literature_as_json(full_literature_data, unique_results)
        
        # ç»„åˆä¸¤éƒ¨åˆ†æ•°æ®
        if json_context:
            combined_context = f"""
ã€ç¬¬ä¸€éƒ¨åˆ†ï¼šRAGå‘é‡æœç´¢ç»“æœã€‘
{context_text}

ã€ç¬¬äºŒéƒ¨åˆ†ï¼šå®Œæ•´æ–‡çŒ®ä¿¡æ¯ã€‘
{literature_text}

ã€ç¬¬ä¸‰éƒ¨åˆ†ï¼šç»“æ„åŒ–ææ–™æ•°æ®ï¼ˆall.jsonæ ¼å¼ï¼‰ã€‘
{json_context}
"""
            logger.info("ğŸ“Š ä½¿ç”¨ç»„åˆæ ¼å¼ï¼šRAGæœç´¢ç»“æœ + ç»“æ„åŒ–JSONæ•°æ®")
        else:
            combined_context = f"""
ã€RAGæœç´¢ç»“æœã€‘
{context_text}

ã€å®Œæ•´æ–‡çŒ®ä¿¡æ¯ã€‘
{literature_text}
"""
            logger.info("ğŸ“ ä½¿ç”¨æ–‡æœ¬æ ¼å¼ï¼šä»…RAGæœç´¢ç»“æœ")
        
        if not llm_handler.is_api_ready():
            logger.warning("LLMæœªé…ç½®ï¼Œè¿”å›åŸºç¡€æœç´¢ç»“æœ")
            result = self._fallback_to_basic_search(search_results)
            result["original_query"] = query
            result["english_query"] = english_query
            result["dois_found"] = top_dois
            result["literature_count"] = len(full_literature_data)
            return result
        
        # æ­¥éª¤6: è®©LLMåˆ†ææœç´¢ç»“æœå’Œæ–‡çŒ®ä¿¡æ¯å¹¶è¿”å›JSONæ ¼å¼
        structured_result = llm_handler.analyze_and_extract_recommendations(english_query, combined_context)
        
        # æ·»åŠ æŸ¥è¯¢ä¿¡æ¯
        structured_result["original_query"] = query
        structured_result["english_query"] = english_query
        structured_result["dois_found"] = top_dois
        structured_result["literature_count"] = len(full_literature_data)
        
        # æ­¥éª¤7: å¤„ç†æ–°æ ¼å¼çš„JSONç»“æ„
        if structured_result.get("content") and isinstance(structured_result["content"], list):
            # æ–°çš„JSONæ ¼å¼ï¼šæå–åŒ–å­¦å¼è¿›è¡Œæ•°æ®åº“å¢å¼º
            content_list = structured_result["content"]
            enhanced_content = []
            
            for item in content_list:
                if "material" in item and "chemical_formula" in item["material"]:
                    formula = item["material"]["chemical_formula"]
                    # ä¸ºè¿™ä¸ªåŒ–å­¦å¼åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„æ—§æ ¼å¼å¯¹è±¡æ¥ä½¿ç”¨ç°æœ‰çš„å¢å¼ºé€»è¾‘
                    temp_formula = {"formula": formula}
                    enhanced_formula = self._enhance_formulas_with_data([temp_formula])
                    
                    # å°†å¢å¼ºçš„éªŒè¯ä¿¡æ¯æ·»åŠ åˆ°ææ–™ä¿¡æ¯ä¸­
                    if enhanced_formula and len(enhanced_formula) > 0:
                        validation = enhanced_formula[0].get('validation', {})
                        if validation:
                            item["material"]["validation"] = validation
                            item["material"]["has_experimental_data"] = validation.get('found_in_db', False)
                
                enhanced_content.append(item)
            
            structured_result["content"] = enhanced_content
            logger.info(f"âœ… å®Œæˆç›´æ¥æ¨èï¼ˆæ–°æ ¼å¼ï¼‰ï¼Œè¿”å› {len(enhanced_content)} ä¸ªææ–™")
        elif structured_result.get("chemical_formulas"):
            # å…¼å®¹æ—§æ ¼å¼
            enhanced_formulas = self._enhance_formulas_with_data(structured_result["chemical_formulas"])
            structured_result["chemical_formulas"] = enhanced_formulas
            logger.info(f"âœ… å®Œæˆç›´æ¥æ¨èï¼ˆæ—§æ ¼å¼ï¼‰ï¼Œè¿”å› {len(structured_result.get('chemical_formulas', []))} ä¸ªåŒ–å­¦å¼")
        else:
            logger.info("âœ… å®Œæˆæ¨èï¼Œä½†æœªæ‰¾åˆ°æœ‰æ•ˆçš„ææ–™ä¿¡æ¯")
        
        return structured_result
    
    def _format_search_results_for_llm(self, search_results: List[Dict[str, Any]]) -> str:
        """å°†æœç´¢ç»“æœæ ¼å¼åŒ–ä¸ºLLMå¯ç†è§£çš„æ–‡æœ¬"""
        context_parts = []
        
        for i, result in enumerate(search_results, 1):
            doc = result.get('document', '')
            meta = result.get('metadata', {})
            score = result.get('similarity_score', 0)
            
            context_part = f"æ–‡æ¡£ {i} (ç›¸ä¼¼åº¦: {score:.3f}):\n"
            context_part += f"æ ‡é¢˜: {meta.get('title', 'æœªçŸ¥')}\n"
            context_part += f"å†…å®¹: {doc[:500]}...\n"  # é™åˆ¶é•¿åº¦é¿å…è¶…å‡ºtokené™åˆ¶
            if meta.get('doi'):
                context_part += f"DOI: {meta['doi']}\n"
            context_part += "\n"
            
            context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    def _fallback_to_basic_search(self, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å½“LLMä¸å¯ç”¨æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ"""
        # ç®€å•æå–ä¸€äº›åŸºæœ¬ä¿¡æ¯
        formulas = []
        methods = []
        procedures = []
        
        for result in search_results[:3]:  # åªå–å‰3ä¸ªç»“æœ
            meta = result.get('metadata', {})
            doc = result.get('document', '')
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…æ¥åˆ†ç±»
            if meta.get('type') == 'chemical_formula' or 'formula' in meta.get('value', '').lower():
                formulas.append({
                    "formula": meta.get('value', 'æœªçŸ¥'),
                    "source": meta.get('title', 'æœªçŸ¥'),
                    "doi": meta.get('doi', ''),
                    "confidence": "medium"
                })
            elif 'synthesis' in doc.lower() or 'preparation' in doc.lower():
                methods.append({
                    "method": doc[:100] + "...",
                    "source": meta.get('title', 'æœªçŸ¥'),
                    "doi": meta.get('doi', ''),
                    "confidence": "medium"
                })
            elif 'test' in doc.lower() or 'measurement' in doc.lower():
                procedures.append({
                    "procedure": doc[:100] + "...",
                    "source": meta.get('title', 'æœªçŸ¥'),
                    "doi": meta.get('doi', ''),
                    "confidence": "medium"
                })
        
        return {
            "chemical_formulas": formulas,
            "synthesis_methods": methods,
            "testing_procedures": procedures,
            "fallback": True
        }
    
    def _enhance_formulas_with_data(self, formulas: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢æˆ–AIé¢„æµ‹å¢å¼ºåŒ–å­¦å¼ä¿¡æ¯"""
        enhanced_formulas = []
        
        for formula_item in formulas:
            formula = formula_item.get('formula', '')
            if not formula:
                continue
            
            # æŸ¥è¯¢æ•°æ®åº“æˆ–è¿›è¡Œé¢„æµ‹
            if self.material_validator:
                validation_result = self.material_validator.validate_material(formula, enable_prediction=True)
                enhanced_item = {
                    **formula_item,
                    'validation': validation_result,
                    'has_experimental_data': validation_result.get('found_in_db', False),
                    'performance_data': validation_result.get('experimental_data') or validation_result.get('prediction')
                }
            else:
                enhanced_item = formula_item
            
            enhanced_formulas.append(enhanced_item)
        
        return enhanced_formulas

    def _get_unique_top_results(self, search_results: List[Dict[str, Any]], max_results: int = 2) -> Tuple[List[Dict[str, Any]], List[str]]:
        """å¯¹æœç´¢ç»“æœè¿›è¡Œå»é‡ï¼Œè¿”å›å‰Nä¸ªå”¯ä¸€ç»“æœå’Œå¯¹åº”çš„DOIåˆ—è¡¨"""
        unique_results = []
        dois = []
        seen_identifiers = set()
        seen_dois = set()
        
        for result in search_results:
            meta = result.get('metadata', {})
            
            # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦ï¼šä¼˜å…ˆä½¿ç”¨DOIï¼Œå…¶æ¬¡ä½¿ç”¨æ ‡é¢˜
            identifier = None
            if meta.get('doi'):
                identifier = meta['doi'].strip().lower()
            elif meta.get('title'):
                identifier = meta['title'].strip().lower()
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œä½¿ç”¨å†…å®¹çš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºæ ‡è¯†ç¬¦
                doc = result.get('document', '')
                identifier = doc[:50].strip().lower()
            
            # å¦‚æœæ˜¯å”¯ä¸€çš„ç»“æœï¼Œåˆ™æ·»åŠ 
            if identifier and identifier not in seen_identifiers:
                seen_identifiers.add(identifier)
                unique_results.append(result)
                
                # åŒæ—¶æå–DOI
                doi = meta.get('doi', '')
                if doi and doi not in seen_dois:
                    # æ¸…ç†å’ŒéªŒè¯DOI
                    if doi.startswith('http'):
                        # æå–DOIéƒ¨åˆ†ï¼Œå¦‚ https://doi.org/10.1016/xxx -> 10.1016/xxx
                        if '/10.' in doi:
                            doi = doi.split('/10.', 1)[1]
                            doi = '10.' + doi
                    elif not doi.startswith('10.'):
                        # è·³è¿‡ä¸æ˜¯æ ‡å‡†DOIæ ¼å¼çš„
                        doi = None
                    
                    if doi:
                        dois.append(doi)
                        seen_dois.add(doi)
                
                # åªä¿ç•™å‰Nä¸ªå»é‡åçš„ç»“æœ
                if len(unique_results) >= max_results:
                    break
        
        return unique_results, dois

    def _extract_top_dois(self, search_results: List[Dict[str, Any]], max_dois: int = 2) -> List[str]:
        """ä»æœç´¢ç»“æœä¸­æå–å‰Nä¸ªæœ€åŒ¹é…çš„DOI"""
        dois = []
        seen_dois = set()
        
        for result in search_results:
            meta = result.get('metadata', {})
            doi = meta.get('doi', '')
            
            # æ¸…ç†å’ŒéªŒè¯DOI
            if doi and doi not in seen_dois:
                # ç®€å•æ¸…ç†DOIæ ¼å¼
                if doi.startswith('http'):
                    # æå–DOIéƒ¨åˆ†ï¼Œå¦‚ https://doi.org/10.1016/xxx -> 10.1016/xxx
                    if '/10.' in doi:
                        doi = doi.split('/10.', 1)[1]
                        doi = '10.' + doi
                elif not doi.startswith('10.'):
                    # è·³è¿‡ä¸æ˜¯æ ‡å‡†DOIæ ¼å¼çš„
                    continue
                
                dois.append(doi)
                seen_dois.add(doi)
                
                if len(dois) >= max_dois:
                    break
        
        return dois
    
    def _get_literature_by_dois(self, dois: List[str]) -> List[Dict[str, Any]]:
        """æ ¹æ®DOIåˆ—è¡¨ä»data/all_mxene.jsonä¸­è·å–å®Œæ•´æ–‡çŒ®ä¿¡æ¯"""
        if not dois:
            return []
        
        # åŠ è½½all_mxene.jsonæ•°æ®
        all_data = self.data_loader.load_all_mxene_json()
        if not all_data:
            logger.warning("æ— æ³•åŠ è½½all_mxene.jsonæ•°æ®")
            return []
        
        found_literature = []
        seen_dois = set()  # ç”¨äºå»é‡
        
        for doi in dois:
            # è·³è¿‡å·²ç»å¤„ç†è¿‡çš„DOI
            if doi in seen_dois:
                logger.info(f"ğŸ”„ DOI {doi} å·²å¤„ç†è¿‡ï¼Œè·³è¿‡é‡å¤")
                continue
            
            # åœ¨all_mxene.jsonä¸­æœç´¢åŒ¹é…çš„DOI
            matching_papers = []
            for paper in all_data:
                paper_doi = paper.get('doi', '')
                
                # å¤šç§DOIåŒ¹é…æ–¹å¼
                if self._is_doi_match(paper_doi, doi):
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æ·»åŠ è¿‡è¿™ç¯‡æ–‡çŒ®ï¼ˆåŸºäºDOIå’Œæ ‡é¢˜ï¼‰
                    paper_id = self._get_paper_id(paper)
                    if not any(self._get_paper_id(existing) == paper_id for existing in found_literature):
                        matching_papers.append(paper)
            
            if matching_papers:
                logger.info(f"ğŸ“– DOI {doi} æ‰¾åˆ° {len(matching_papers)} æ¡æ–‡çŒ®")
                found_literature.extend(matching_papers)
                seen_dois.add(doi)
            else:
                logger.warning(f"âš ï¸ DOI {doi} æœªåœ¨all_mxene.jsonä¸­æ‰¾åˆ°åŒ¹é…æ–‡çŒ®")
        
        # æœ€ç»ˆå»é‡æ£€æŸ¥
        unique_literature = self._deduplicate_literature(found_literature)
        if len(unique_literature) != len(found_literature):
            logger.info(f"ğŸ§¹ å»é‡å®Œæˆï¼š{len(found_literature)} â†’ {len(unique_literature)} ç¯‡æ–‡çŒ®")
        
        return unique_literature
    
    def _is_doi_match(self, paper_doi: str, target_doi: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªDOIæ˜¯å¦åŒ¹é…"""
        if not paper_doi or not target_doi:
            return False
        
        # æ¸…ç†å’Œæ ‡å‡†åŒ–DOI
        def clean_doi(doi):
            doi = doi.strip().lower()
            if 'doi.org/' in doi:
                doi = doi.split('doi.org/')[-1]
            elif doi.startswith('http'):
                if '/10.' in doi:
                    doi = doi.split('/10.', 1)[1]
                    doi = '10.' + doi
            return doi
        
        clean_paper_doi = clean_doi(paper_doi)
        clean_target_doi = clean_doi(target_doi)
        
        return clean_paper_doi == clean_target_doi
    
    def _get_paper_id(self, paper: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡çŒ®çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
        doi = paper.get('doi', '')
        title = paper.get('title', '')
        authors = paper.get('authors', '')
        
        # ä½¿ç”¨DOIã€æ ‡é¢˜å’Œä½œè€…çš„ç»„åˆä½œä¸ºå”¯ä¸€æ ‡è¯†
        if doi:
            # æ¸…ç†DOIæ ¼å¼ç”¨ä½œæ ‡è¯†
            clean_doi = doi.lower().strip()
            if 'doi.org/' in clean_doi:
                clean_doi = clean_doi.split('doi.org/')[-1]
            return f"doi:{clean_doi}"
        elif title:
            return f"title:{title.lower().strip()[:100]}"  # ä½¿ç”¨æ ‡é¢˜å‰100å­—ç¬¦
        else:
            return f"authors:{authors.lower().strip()[:50]}"  # ä½¿ç”¨ä½œè€…å‰50å­—ç¬¦
    
    def _deduplicate_literature(self, literature: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é™¤é‡å¤çš„æ–‡çŒ®"""
        seen_ids = set()
        unique_literature = []
        
        for paper in literature:
            paper_id = self._get_paper_id(paper)
            if paper_id not in seen_ids:
                unique_literature.append(paper)
                seen_ids.add(paper_id)
            else:
                title = paper.get('title', 'æœªçŸ¥')[:50]
                logger.debug(f"ğŸ—‘ï¸ è·³è¿‡é‡å¤æ–‡çŒ®: {title}...")
        
        return unique_literature
    
    def _format_literature_for_llm(self, literature_data: List[Dict[str, Any]]) -> str:
        """å°†å®Œæ•´æ–‡çŒ®æ•°æ®æ ¼å¼åŒ–ä¸ºLLMå¯ç†è§£çš„æ–‡æœ¬"""
        if not literature_data:
            return "æœªæ‰¾åˆ°å®Œæ•´æ–‡çŒ®ä¿¡æ¯ã€‚"
        
        literature_parts = []
        
        for i, paper in enumerate(literature_data, 1):
            literature_part = f"å®Œæ•´æ–‡çŒ® {i}:\n"
            literature_part += f"æ ‡é¢˜: {paper.get('title', 'æœªçŸ¥')}\n"
            literature_part += f"ä½œè€…: {paper.get('authors', 'æœªçŸ¥')}\n"
            literature_part += f"æœŸåˆŠ: {paper.get('journal', 'æœªçŸ¥')}\n"
            literature_part += f"å¹´ä»½: {paper.get('year', 'æœªçŸ¥')}\n"
            literature_part += f"DOI: {paper.get('doi', 'æœªçŸ¥')}\n"
            
            # æ‘˜è¦
            if paper.get('abstract'):
                literature_part += f"æ‘˜è¦: {paper['abstract'][:500]}...\n"
            
            # å…³é”®è¯
            if paper.get('keywords'):
                literature_part += f"å…³é”®è¯: {', '.join(paper['keywords'])}\n"
            
            # åŒ–å­¦å¼ä¿¡æ¯
            if paper.get('chemical_formula'):
                literature_part += f"åŒ–å­¦å¼: {paper['chemical_formula']}\n"
            
            # åˆæˆæ–¹æ³•ä¿¡æ¯  
            if paper.get('synthesis_method'):
                literature_part += f"åˆæˆæ–¹æ³•: {paper['synthesis_method']}\n"
            
            # æµ‹è¯•æ–¹æ³•ä¿¡æ¯
            if paper.get('testing_procedure'):
                literature_part += f"æµ‹è¯•æµç¨‹: {paper['testing_procedure']}\n"
            
            # æ€§èƒ½æ•°æ®
            if paper.get('performance_data'):
                literature_part += f"æ€§èƒ½æ•°æ®: {paper['performance_data']}\n"
            
            literature_part += "\n"
            literature_parts.append(literature_part)
        
        return "\n".join(literature_parts)
    
    def _format_literature_as_json(self, literature_data: List[Dict[str, Any]], search_results: List[Dict[str, Any]]) -> str:
        """å°†æ–‡çŒ®æ•°æ®æ ¼å¼åŒ–ä¸ºJSONæ ¼å¼ä¾›æ–°promptä½¿ç”¨"""
        if not literature_data:
            return None
            
        # åŠ è½½all.jsonæ•°æ®
        all_json_data = self.data_loader.load_all_json()
        if not all_json_data:
            logger.warning("æ— æ³•åŠ è½½all.json")
            return None
        
        # æ”¶é›†æ‰€æœ‰åŒ¹é…DOIçš„JSONè®°å½•
        matching_json_records = []
        
        for paper in literature_data:
            paper_doi = paper.get('doi', '')
            if not paper_doi:
                continue
                
            # åœ¨all.jsonä¸­æ‰¾åˆ°æ‰€æœ‰åŒ¹é…è¿™ä¸ªDOIçš„è®°å½•
            for json_record in all_json_data:
                record_doi = json_record.get('doi', '')
                
                # DOIåŒ¹é…æ£€æŸ¥
                if self._is_doi_match(record_doi, paper_doi):
                    # ç›´æ¥æ·»åŠ å®Œæ•´çš„JSONè®°å½•ï¼Œä¸åšä»»ä½•ä¿®æ”¹
                    matching_json_records.append(json_record)
        
        if matching_json_records:
            # å°†åŒ¹é…çš„JSONè®°å½•å®Œæ•´è¾“å‡º
            import json
            try:
                formatted_json = json.dumps(matching_json_records, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ“Š æ‰¾åˆ° {len(matching_json_records)} ä¸ªåŒ¹é…çš„JSONè®°å½•")
                return f"all.jsonä¸­åŒ¹é…çš„å®Œæ•´è®°å½•ï¼š\n{formatted_json}"
            except Exception as e:
                logger.warning(f"JSONåºåˆ—åŒ–å¤±è´¥: {e}")
                return None
        
        logger.info("æœªæ‰¾åˆ°åŒ¹é…çš„JSONè®°å½•")
        return None
    
    def _extract_json_from_text(self, text: str) -> Dict[str, Any]:
        """ä»æ–‡æœ¬ä¸­æå–JSONæ ¼å¼çš„ææ–™æ•°æ®"""
        import re
        import json
        
        # æŸ¥æ‰¾JSONå¯¹è±¡çš„æ¨¡å¼
        json_patterns = [
            r'\{[^{}]*"chemical_formula"[^{}]*\}',  # ç®€å•çš„å•å±‚JSON
            r'\{(?:[^{}]|{[^{}]*})*"chemical_formula"(?:[^{}]|{[^{}]*})*\}',  # åŒ…å«åµŒå¥—çš„JSON
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                try:
                    # å°è¯•è§£æJSON
                    data = json.loads(match)
                    if isinstance(data, dict) and 'chemical_formula' in data:
                        return data
                except:
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®Œæ•´çš„JSONï¼Œå°è¯•æå–å…³é”®ä¿¡æ¯æ„å»ºåŸºæœ¬JSON
        return self._construct_basic_json_from_text(text)
    
    def _construct_basic_json_from_text(self, text: str) -> Dict[str, Any]:
        """ä»æ–‡æœ¬ä¸­æ„å»ºåŸºæœ¬çš„JSONç»“æ„"""
        import re
        
        # æå–åŒ–å­¦å¼
        formula_patterns = [
            r'\b[A-Z][a-z]?\d*[A-Z][a-z]?\d*T?x?\b',  # åŸºæœ¬MXeneæ¨¡å¼å¦‚Ti3C2Tx
            r'\b[A-Z][a-z]?\d*C\d*T?x?\b',  # ç¢³åŒ–ç‰©æ¨¡å¼
            r'\b[A-Z][a-z]?-MXene\b',  # æ˜ç¡®çš„MXeneæ ‡è®°
        ]
        
        formulas = []
        for pattern in formula_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            formulas.extend(matches)
        
        if not formulas:
            return None
            
        # æ„å»ºåŸºæœ¬JSONç»“æ„
        basic_json = {
            "material": {
                "chemical_formula": formulas[0] if formulas else "æœªçŸ¥"
            }
        }
        
        # å°è¯•æå–æ€§èƒ½æ•°æ®
        performance = {}
        
        # æŸ¥æ‰¾RLå€¼
        rl_patterns = [
            r'RL.*?(-?\d+\.?\d*)\s*dB',
            r'reflection\s*loss.*?(-?\d+\.?\d*)\s*dB',
            r'åå°„æŸè€—.*?(-?\d+\.?\d*)\s*dB'
        ]
        
        for pattern in rl_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                performance['rl_min'] = f"{match.group(1)} dB"
                break
        
        # æŸ¥æ‰¾EABå€¼
        eab_patterns = [
            r'EAB.*?(\d+\.?\d*)\s*GHz',
            r'effective\s*absorption\s*bandwidth.*?(\d+\.?\d*)\s*GHz',
            r'æœ‰æ•ˆå¸æ”¶å¸¦å®½.*?(\d+\.?\d*)\s*GHz'
        ]
        
        for pattern in eab_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                performance['eab'] = f"{match.group(1)} GHz"
                break
        
        if performance:
            basic_json['performance'] = performance
        
        return basic_json
